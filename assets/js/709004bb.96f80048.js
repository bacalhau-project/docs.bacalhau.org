"use strict";(self.webpackChunkbacalhau_docs=self.webpackChunkbacalhau_docs||[]).push([[8129],{1608:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var o=n(5893),a=n(1151);const i={sidebar_label:"CLI Reference",sidebar_position:7},s="CLI Commands",r={id:"all-flags",title:"CLI Commands",description:"The following commands refer to bacalhau cli version v1.0.3.",source:"@site/docs/all-flags.md",sourceDirName:".",slug:"/all-flags",permalink:"/all-flags",draft:!1,unlisted:!1,editUrl:"https://github.com/bacalhau-project/docs.bacalhau.org/blob/main/docs/all-flags.md",tags:[],version:"current",lastUpdatedAt:1700797095,formattedLastUpdatedAt:"Nov 24, 2023",sidebarPosition:7,frontMatter:{sidebar_label:"CLI Reference",sidebar_position:7},sidebar:"documentationSidebar",previous:{title:"FAQs",permalink:"/troubleshooting/faqs"},next:{title:"Integration",permalink:"/integration"}},l={},c=[{value:"Cancel",id:"cancel",level:2},{value:"Examples",id:"examples",level:4},{value:"Create",id:"create",level:2},{value:"Examples",id:"examples-1",level:4},{value:"UCAN Invocation format",id:"ucan-invocation-format",level:3},{value:"Examples",id:"examples-2",level:4},{value:"Describe",id:"describe",level:2},{value:"Example",id:"example",level:4},{value:"Docker run",id:"docker-run",level:2},{value:"Get",id:"get",level:2},{value:"Example",id:"example-1",level:4},{value:"List",id:"list",level:2},{value:"Example",id:"example-2",level:4},{value:"Logs",id:"logs",level:2},{value:"Examples",id:"examples-3",level:4},{value:"Run Python",id:"run-python",level:2},{value:"Serve",id:"serve",level:2}];function u(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",...(0,a.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h1,{id:"cli-commands",children:"CLI Commands"}),"\n",(0,o.jsx)(t.admonition,{type:"info",children:(0,o.jsxs)(t.p,{children:["The following commands refer to bacalhau cli version ",(0,o.jsx)(t.code,{children:"v1.0.3"}),".\nFor installing or upgrading a client, follow the instructions in the ",(0,o.jsx)(t.a,{href:"https://docs.bacalhau.org/getting-started/installation",children:"installation page"}),".\nRun ",(0,o.jsx)(t.code,{children:"bacalhau version"})," in a terminal to check what version you have."]})}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'\u276f bacalhau --help\nCompute over data\n\nUsage:\n  bacalhau [command]\n\nAvailable Commands:\n  cancel      Cancel a previously submitted job\n  completion  Generate the autocompletion script for the specified shell\n  create      Create a job using a json or yaml file.\n  describe    Describe a job on the network\n  devstack    Start a cluster of bacalhau nodes for testing and development\n  docker      Run a docker job on the network (see run subcommand)\n  get         Get the results of a job\n  help        Help about any command\n  id          Show bacalhau node id info\n  list        List jobs on the network\n  logs        Follow logs from a currently executing job\n  run         Run a job on the network (see subcommands for supported flavors)\n  serve       Start the bacalhau compute node\n  validate    validate a job using a json or yaml file.\n  version     Get the client and server version.\n\nFlags:\n      --api-host string   The host for the client and server to communicate on (via REST). Ignored if BACALHAU_API_HOST environment variable is set. (default "bootstrap.production.bacalhau.org")\n      --api-port int      The port for the client and server to communicate on (via REST). Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n  -h, --help              help for bacalhau\n\nUse "bacalhau [command] --help" for more information about a command.\n'})}),"\n",(0,o.jsx)(t.h2,{id:"cancel",children:"Cancel"}),"\n",(0,o.jsx)(t.p,{children:"Cancels a job that was previously submitted and stops it running if it has not yet completed."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"Cancel a previously submitted job.\n\nUsage:\n  ./bin/darwin_arm64/bacalhau cancel [id] [flags]\n\nFlags:\n  -h, --help    help for cancel\n      --quiet   Do not print anything to stdout or stderr\n"})}),"\n",(0,o.jsx)(t.h4,{id:"examples",children:"Examples"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"Examples:\n  # Cancel a previously submitted job\n  bacalhau cancel 51225160-807e-48b8-88c9-28311c7899e1\n\n  # Cancel a job, with a short ID.\n  bacalhau cancel ebd9bf2f\n"})}),"\n",(0,o.jsx)(t.h2,{id:"create",children:"Create"}),"\n",(0,o.jsx)(t.p,{children:"Submit a job to the network in a declarative way by writing a jobspec instead of writing a command.\nJSON and YAML formats are accepted."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'Create a job from a file or from stdin.\n\n JSON and YAML formats are accepted.\n\nUsage:\n  bacalhau create [flags]\n\nFlags:\n      --download                    Download the results and print stdout once the job has completed (implies --wait).\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 10)\n  -g, --gettimeout int              Timeout for getting the results of a job in --wait (default 10)\n  -h, --help                        help for create\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --local                       Run the job locally. Docker is required\n      --output-dir string           Directory to write the output to. (default ".")\n      --wait                        Wait for the job to finish. Use --wait=false to not wait.\n      --wait-timeout-secs int       When using --wait, how many seconds to wait for the job to complete before giving up. (default 600)\n'})}),"\n",(0,o.jsx)(t.h4,{id:"examples-1",children:"Examples"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"Examples:\n  # Create a job using the data in job.yaml\n  bacalhau create ./job.yaml\n\n  # Create a new job from an already executed job\n  bacalhau describe 6e51df50 | bacalhau create -\n"})}),"\n",(0,o.jsx)(t.p,{children:"An example job in YAML format:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-yaml",children:"spec:\n    engine: Docker\n    verifier: Noop\n    publisher: IPFS\n    docker:\n        image: ubuntu\n        entryPoint:\n            - echo\n        parameters:\n            - Hello\n            - World\n    outputs:\n        - name: outputs\n          path: /outputs\ndeal:\n    concurrency: 1\n"})}),"\n",(0,o.jsx)(t.h3,{id:"ucan-invocation-format",children:"UCAN Invocation format"}),"\n",(0,o.jsxs)(t.p,{children:["You can also specify a job to run using a ",(0,o.jsx)(t.a,{href:"https://github.com/ucan-wg/invocation",children:"UCAN Invocation"})," object in JSON format. For the fields supported by Bacalhau, see the ",(0,o.jsx)(t.a,{href:"https://github.com/bacalhau-project/bacalhau/blob/main/pkg/model/schemas/bacalhau.ipldsch",children:"IPLD schema"}),"."]}),"\n",(0,o.jsx)(t.p,{children:"There is no support for sharding, concurrency or minimum bidding for these jobs."}),"\n",(0,o.jsx)(t.h4,{id:"examples-2",children:"Examples"}),"\n",(0,o.jsxs)(t.p,{children:["Refers to example models at balcalhau repository under ",(0,o.jsx)(t.a,{href:"https://github.com/bacalhau-project/bacalhau/tree/main/pkg/model/tasks",children:"pkg/model/tasks"})]}),"\n",(0,o.jsx)(t.p,{children:"An example UCAN Invocation that runs the same job as the above example would look like:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-json",children:'{\n  "with": "ubuntu",\n  "do": "docker/run",\n  "inputs": {\n    "entrypoint": ["echo"],\n    "parameters": ["hello", "world"],\n    "workdir": "/",\n    "mounts": {},\n    "outputs": {\n      "/outputs": ""\n    }\n  },\n  "meta": {\n    "bacalhau/config": {\n      "verifier": 1,\n      "publisher": 4,\n      "annotations": ["hello"],\n      "resources": {\n        "cpu": 1,\n        "disk": 1073741824,\n        "memory": 1073741824,\n        "gpu": 0\n      },\n      "timeout": 300e9,\n      "dnt": false\n    }\n  }\n}\n'})}),"\n",(0,o.jsx)(t.p,{children:"An example UCAN Invocation that runs a WebAssembly job might look like:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-json",children:'{\n\t"with": "ipfs://bafybeig7mdkzcgpacpozamv7yhhaelztfrnb6ozsupqqh7e5uyqdkijegi",\n\t"do": "wasm32-wasi/run",\n\t"inputs": {\n\t\t"entrypoint": "_start",\n\t\t"parameters": ["/inputs/data.tar.gz"],\n\t\t"mounts": {\n\t\t\t"/inputs": "https://www.example.com/data.tar.gz"\n\t\t},\n\t\t"outputs": {\n\t\t\t"/outputs": ""\n\t\t},\n\t\t"env": {\n\t\t\t"HELLO": "world"\n\t\t}\n\t},\n\t"meta": {\n    }\n  }\n}\n'})}),"\n",(0,o.jsx)(t.h2,{id:"describe",children:"Describe"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"Full description of a job, in yaml format. Use 'bacalhau list' to get a list of all ids. Short form and long form of the job id are accepted.\n\nUsage:\n  bacalhau describe [id] [flags]\n\nFlags:\n  -h, --help             help for describe\n      --include-events   Include events in the description (could be noisy)\n      --spec             Output Jobspec to stdout\n"})}),"\n",(0,o.jsx)(t.h4,{id:"example",children:"Example"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"Examples:\n  # Describe a job with the full ID\n  bacalhau describe e3f8c209-d683-4a41-b840-f09b88d087b9\n\n  # Describe a job with the a shortened ID\n  bacalhau describe 47805f5c\n\n  # Describe a job and include all server and local events\n  bacalhau describe --include-events b6ad164a\n"})}),"\n",(0,o.jsx)(t.h2,{id:"docker-run",children:"Docker run"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"Runs a job using the Docker executor on the node.\n\nUsage:\n  bacalhau docker run [flags] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\n\nExamples:\n  # Run a Docker job, using the image 'dpokidov/imagemagick', with a CID mounted at /input_images and an output volume mounted at /outputs in the container. All flags after the '--' are passed directly into the container for execution.\n  bacalhau docker run \\\n  -i src=ipfs://QmeZRGhe4PmjctYVSVHuEiA9oSXnqmYa4kQubSHgWbjv72,dst=/input_images \\\n  dpokidov/imagemagick:7.1.0-47-ubuntu \\\n  -- magick mogrify -resize 100x100 -quality 100 -path /outputs '/input_images/*.jpg'\n\n  # Dry Run: check the job specification before submitting it to the bacalhau network\n  bacalhau docker run --dry-run ubuntu echo hello\n\n  # Save the job specification to a YAML file\n  bacalhau docker run --dry-run ubuntu echo hello > job.yaml\n\n  # Specify an image tag (default is 'latest' - using a specific tag other than 'latest' is recommended for reproducibility)\n  bacalhau docker run ubuntu:bionic echo hello\n\n  # Specify an image digest\n  bacalhau docker run ubuntu@sha256:35b4f89ec2ee42e7e12db3d107fe6a487137650a2af379bbd49165a1494246ea echo hello\n\nFlags:\n  -c, --concurrency int                  How many nodes should run the job (default 1)\n      --confidence int                   The minimum number of nodes that must agree on a verification result\n      --cpu string                       Job CPU cores (e.g. 500m, 2, 8).\n      --domain stringArray               Domain(s) that the job needs to access (for HTTP networking)\n      --download                         Should we download the results once the job is complete?\n      --download-timeout-secs duration   Timeout duration for IPFS downloads. (default 5m0s)\n      --dry-run                          Do not submit the job, but instead print out what will be submitted\n      --engine string                    What executor engine to use to run the job (default \"docker\")\n  -e, --env strings                      The environment variables to supply to the job (e.g. --env FOO=bar --env BAR=baz)\n      --filplus                          Mark the job as a candidate for moderation for FIL+ rewards.\n  -f, --follow                           When specified will follow the output from the job as it runs\n  -g, --gettimeout int                   Timeout for getting the results of a job in --wait (default 10)\n      --gpu string                       Job GPU requirement (e.g. 1, 2, 8).\n  -h, --help                             help for run\n      --id-only                          Print out only the Job ID on successful submission.\n  -i, --input storage                    Mount URIs as inputs to the job. Can be specified multiple times. Format: src=URI,dst=PATH[,opt=key=value]\n                                         Examples:\n                                         # Mount IPFS CID to /inputs directory\n                                         -i ipfs://QmeZRGhe4PmjctYVSVHuEiA9oSXnqmYa4kQubSHgWbjv72\n\n                                         # Mount S3 object to a specific path\n                                         -i s3://bucket/key,dst=/my/input/path\n\n                                         # Mount S3 object with specific endpoint and region\n                                         -i src=s3://bucket/key,dst=/my/input/path,opt=endpoint=https://s3.example.com,opt=region=us-east-1\n\n      --ipfs-swarm-addrs string          Comma-separated list of IPFS nodes to connect to. (default \"/ip4/35.245.115.191/tcp/1235/p2p/QmdZQ7ZbhnvWY1J12XYKGHApJ6aufKyLNSvf8jZBrBaAVL,/ip4/35.245.61.251/tcp/1235/p2p/QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF,/ip4/35.245.251.239/tcp/1235/p2p/QmYgxZiySj3MRkwLSL4X2MF5F9f2PMhAE3LV49XkfNL1o3\")\n  -l, --labels strings                   List of labels for the job. Enter multiple in the format '-l a -l 2'. All characters not matching /a-zA-Z0-9_:|-/ and all emojis will be stripped.\n      --local                            Run the job locally. Docker is required\n      --memory string                    Job Memory requirement (e.g. 500Mb, 2Gb, 8Gb).\n      --min-bids int                     Minimum number of bids that must be received before concurrency-many bids will be accepted (at random)\n      --network network-type             Networking capability required by the job (default None)\n      --node-details                     Print out details of all nodes (overridden by --id-only).\n      --output-dir string                Directory to write the output to.\n  -o, --output-volumes strings           name:path of the output data volumes. 'outputs:/outputs' is always added.\n  -p, --publisher publisher              Where to publish the result of the job (default Estuary)\n      --raw                              Download raw result CIDs instead of merging multiple CIDs into a single result\n  -s, --selector string                  Selector (label query) to filter nodes on which this job can be executed, supports '=', '==', and '!='.(e.g. -s key1=value1,key2=value2). Matching objects must satisfy all of the specified label constraints.\n      --skip-syntax-checking             Skip having 'shellchecker' verify syntax of the command\n      --timeout float                    Job execution timeout in seconds (e.g. 300 for 5 minutes and 0.1 for 100ms) (default 1800)\n      --verifier string                  What verification engine to use to run the job (default \"noop\")\n      --wait                             Wait for the job to finish. (default true)\n      --wait-timeout-secs int            When using --wait, how many seconds to wait for the job to complete before giving up. (default 600)\n  -w, --workdir string                   Working directory inside the container. Overrides the working directory shipped with the image (e.g. via WORKDIR in Dockerfile).\n"})}),"\n",(0,o.jsx)(t.h2,{id:"get",children:"Get"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'Get the results of the job, including `stdout` and `stderr`.\n\nUsage:\n  bacalhau get [id] [flags]\n\nFlags:\n      --download-timeout-secs int   Timeout duration for IPFS downloads. (default 600)\n  -h, --help                        help for get\n      --ipfs-swarm-addrs string     Comma-separated list of IPFS nodes to connect to.\n      --output-dir string           Directory to write the output to. (default ".")\n'})}),"\n",(0,o.jsx)(t.h4,{id:"example-1",children:"Example"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"# Get the results of a job.\nbacalhau get 51225160-807e-48b8-88c9-28311c7899e1\n\n# Get the results of a job, with a short ID.\nbacalhau get ebd9bf2f\n"})}),"\n",(0,o.jsx)(t.h2,{id:"list",children:"List"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'List jobs on the network.\n\nUsage:\n  bacalhau list [flags]\n\nFlags:\n      --all                Fetch all jobs from the network (default is to filter those belonging to the user). This option may take a long time to return, please use with caution.\n  -h, --help               help for list\n      --hide-header        do not print the column headers.\n      --id-filter string   filter by Job List to IDs matching substring.\n      --no-style           remove all styling from table output.\n  -n, --number int         print the first NUM jobs instead of the first 10. (default 10)\n      --output string      The output format for the list of jobs (json or text) (default "text")\n      --reverse            reverse order of table - for time sorting, this will be newest first. Use \'--reverse=false\' to sort oldest first (single quotes are required). (default true)\n      --sort-by Column     sort by field, defaults to creation time, with newest first [Allowed "id", "created_at"]. (default created_at)\n      --wide               Print full values in the table results\n'})}),"\n",(0,o.jsx)(t.h4,{id:"example-2",children:"Example"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"# List jobs on the network\nbacalhau list\n\n# List jobs and output as json\nbacalhau list --output json\n"})}),"\n",(0,o.jsx)(t.h2,{id:"logs",children:"Logs"}),"\n",(0,o.jsx)(t.p,{children:"Retrieves the log output (stdout, and stderr) from a job.\nIf the job is still running it is possible to follow the logs after the previously generated logs are retrieved."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"Follow logs from a currently executing job\n\nUsage:\n  ./bin/darwin_arm64/bacalhau logs [flags] [id]\n\nFlags:\n  -f, --follow   Follow the logs in real-time after retrieving the current logs.\n  -h, --help     help for logs\n"})}),"\n",(0,o.jsx)(t.h4,{id:"examples-3",children:"Examples"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"Examples:\n  # Follow logs for a previously submitted job\n  bacalhau logs -f 51225160-807e-48b8-88c9-28311c7899e1\n\n  # Retrieve the log output with a short ID, but don't follow any newly generated logs\n  bacalhau logs ebd9bf2f\n"})}),"\n",(0,o.jsx)(t.h2,{id:"run-python",children:"Run Python"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'Runs a job by compiling language file to WASM on the node.\n\nUsage:\n  bacalhau run python [flags]\n\nExamples:\n  # Run a simple "Hello, World" script within the current directory\n  bacalhau run python -- hello-world.py\n\nFlags:\n  -c, --command string                   Program passed in as string (like python)\n      --concurrency int                  How many nodes should run the job (default 1)\n      --confidence int                   The minimum number of nodes that must agree on a verification result\n      --context-path string              Path to context (e.g. python code) to send to server (via public IPFS network) for execution (max 10MiB). Set to empty string to disable (default ".")\n      --deterministic                    Enforce determinism: run job in a single-threaded wasm runtime with no sources of entropy. NB: this will make the python runtime executein an environment where only some libraries are supported, see https://pyodide.org/en/stable/usage/packages-in-pyodide.html (default true)\n      --download                         Should we download the results once the job is complete?\n      --download-timeout-secs duration   Timeout duration for IPFS downloads. (default 5m0s)\n  -e, --env strings                      The environment variables to supply to the job (e.g. --env FOO=bar --env BAR=baz)\n  -f, --follow                           When specified will follow the output from the job as it runs\n  -g, --gettimeout int                   Timeout for getting the results of a job in --wait (default 10)\n  -h, --help                             help for python\n      --id-only                          Print out only the Job ID on successful submission.\n  -i, --input storage                    Mount URIs as inputs to the job. Can be specified multiple times. Format: src=URI,dst=PATH[,opt=key=value]\n                                         Examples:\n                                         # Mount IPFS CID to /inputs directory\n                                         -i ipfs://QmeZRGhe4PmjctYVSVHuEiA9oSXnqmYa4kQubSHgWbjv72\n\n                                         # Mount S3 object to a specific path\n                                         -i s3://bucket/key,dst=/my/input/path\n\n                                         # Mount S3 object with specific endpoint and region\n                                         -i src=s3://bucket/key,dst=/my/input/path,opt=endpoint=https://s3.example.com,opt=region=us-east-1\n\n      --ipfs-swarm-addrs string          Comma-separated list of IPFS nodes to connect to. (default "/ip4/35.245.115.191/tcp/1235/p2p/QmdZQ7ZbhnvWY1J12XYKGHApJ6aufKyLNSvf8jZBrBaAVL,/ip4/35.245.61.251/tcp/1235/p2p/QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF,/ip4/35.245.251.239/tcp/1235/p2p/QmYgxZiySj3MRkwLSL4X2MF5F9f2PMhAE3LV49XkfNL1o3")\n  -l, --labels strings                   List of labels for the job. Enter multiple in the format \'-l a -l 2\'. All characters not matching /a-zA-Z0-9_:|-/ and all emojis will be stripped.\n      --local                            Run the job locally. Docker is required\n      --min-bids int                     Minimum number of bids that must be received before concurrency-many bids will be accepted (at random)\n      --node-details                     Print out details of all nodes (overridden by --id-only).\n      --output-dir string                Directory to write the output to.\n  -o, --output-volumes strings           name:path of the output data volumes\n      --raw                              Download raw result CIDs instead of merging multiple CIDs into a single result\n  -r, --requirement string               Install from the given requirements file. (like pip)\n      --timeout float                    Job execution timeout in seconds (e.g. 300 for 5 minutes and 0.1 for 100ms) (default 1800)\n      --wait                             Wait for the job to finish. (default true)\n      --wait-timeout-secs int            When using --wait, how many seconds to wait for the job to complete before giving up. (default 600)\n'})}),"\n",(0,o.jsx)(t.h2,{id:"serve",children:"Serve"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'Start a bacalhau node.\n\nUsage:\n  bacalhau serve [flags]\n\nExamples:\n  # Start a private bacalhau requester node\n  bacalhau serve\n  # or\n  bacalhau serve --node-type requester\n  \n  # Start a private bacalhau hybrid node that acts as both compute and requester\n  bacalhau serve --node-type compute --node-type requester\n  # or\n  bacalhau serve --node-type compute,requester\n  \n  # Start a private bacalhau node with a persistent local IPFS node\n  BACALHAU_SERVE_IPFS_PATH=/data/ipfs bacalhau serve\n  \n  # Start a public bacalhau requester node\n  bacalhau serve --peer env --private-internal-ipfs=false\n\nFlags:\n      --estuary-api-key string                           The API key used when using the estuary API.\n      --filecoin-unsealed-path string                    The go template that can turn a filecoin CID into a local filepath with the unsealed data.\n  -h, --help                                             help for serve\n      --host string                                      The host to listen on (for both api and swarm connections). (default "0.0.0.0")\n      --ipfs-connect string                              The ipfs host multiaddress to connect to, otherwise an in-process IPFS node will be created if not set.\n      --ipfs-swarm-addr strings                          IPFS multiaddress to connect the in-process IPFS node to - cannot be used with --ipfs-connect.\n      --job-execution-timeout-bypass-client-id strings   List of IDs of clients that are allowed to bypass the job execution timeout check\n      --job-selection-accept-networked                   Accept jobs that require network access.\n      --job-selection-data-locality string               Only accept jobs that reference data we have locally ("local") or anywhere ("anywhere"). (default "local")\n      --job-selection-probe-exec string                  Use the result of a exec an external program to decide if we should take on the job.\n      --job-selection-probe-http string                  Use the result of a HTTP POST to decide if we should take on the job.\n      --job-selection-reject-stateless                   Reject jobs that don\'t specify any data.\n      --labels stringToString                            Labels to be associated with the node that can be used for node selection and filtering. (e.g. --labels key1=value1,key2=value2) (default [])\n      --limit-job-cpu string                             Job CPU core limit for single job (e.g. 500m, 2, 8).\n      --limit-job-gpu string                             Job GPU limit for single job (e.g. 1, 2, or 8).\n      --limit-job-memory string                          Job Memory limit for single job  (e.g. 500Mb, 2Gb, 8Gb).\n      --limit-total-cpu string                           Total CPU core limit to run all jobs (e.g. 500m, 2, 8).\n      --limit-total-gpu string                           Total GPU limit to run all jobs (e.g. 1, 2, or 8).\n      --limit-total-memory string                        Total Memory limit to run all jobs  (e.g. 500Mb, 2Gb, 8Gb).\n      --lotus-max-ping duration                          The highest ping a Filecoin miner could have when selecting. (default 2s)\n      --lotus-path-directory string                      Location of the Lotus Filecoin configuration directory.\n      --lotus-storage-duration duration                  Duration to store data in Lotus Filecoin for.\n      --lotus-upload-directory string                    Directory to use when uploading content to Lotus Filecoin.\n      --node-type strings                                Whether the node is a compute, requester or both. (default [requester])\n      --peer string                                      A comma-separated list of libp2p multiaddress to connect to. Use "none" to avoid connecting to any peer, "env" to connect to the default peer list of your active environment (see BACALHAU_ENVIRONMENT env var). (default "none")\n      --private-internal-ipfs                            Whether the in-process IPFS node should auto-discover other nodes, including the public IPFS network - cannot be used with --ipfs-connect. Use "--private-internal-ipfs=false" to disable. To persist a local Ipfs node, set BACALHAU_SERVE_IPFS_PATH to a valid path. (default true)\n      --swarm-port int                                   The port to listen on for swarm connections. (default 1235)\n\nGlobal Flags:\n      --api-host string         The host for the client and server to communicate on (via REST).\n                                Ignored if BACALHAU_API_HOST environment variable is set. (default "bootstrap.production.bacalhau.org")\n      --api-port uint16         The port for the client and server to communicate on (via REST).\n                                Ignored if BACALHAU_API_PORT environment variable is set. (default 1234)\n      --log-mode logging-mode   Log format: \'default\',\'station\',\'json\',\'combined\',\'event\' (default default)\n'})})]})}function d(e={}){const{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},1151:(e,t,n)=>{n.d(t,{Z:()=>r,a:()=>s});var o=n(7294);const a={},i=o.createContext(a);function s(e){const t=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(i.Provider,{value:t},e.children)}}}]);