"use strict";(self.webpackChunkbacalhau_docs=self.webpackChunkbacalhau_docs||[]).push([[7864],{5827:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var r=t(5893),i=t(1151);const a={sidebar_label:"S3-Model-Inference",sidebar_position:3},s="Running Inference on a Model stored on S3",o={id:"examples/model-inference/S3-Model-Inference/index",title:"Running Inference on a Model stored on S3",description:"Open In Colab",source:"@site/docs/examples/model-inference/S3-Model-Inference/index.md",sourceDirName:"examples/model-inference/S3-Model-Inference",slug:"/examples/model-inference/S3-Model-Inference/",permalink:"/examples/model-inference/S3-Model-Inference/",draft:!1,unlisted:!1,editUrl:"https://github.com/bacalhau-project/docs.bacalhau.org/blob/main/docs/examples/model-inference/S3-Model-Inference/index.md",tags:[],version:"current",lastUpdatedAt:1702044882,formattedLastUpdatedAt:"Dec 8, 2023",sidebarPosition:3,frontMatter:{sidebar_label:"S3-Model-Inference",sidebar_position:3},sidebar:"documentationSidebar",previous:{title:"Object Detection - YOLOv5",permalink:"/examples/model-inference/object-detection-yolo5/"},next:{title:"Stable Diffusion -CKPT",permalink:"/examples/model-inference/Stable-Diffusion-CKPT-Inference/"}},l={},c=[{value:"Running Locally",id:"running-locally",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Downloading the Datasets",id:"downloading-the-datasets",level:3},{value:"Creating the Inference Script",id:"creating-the-inference-script",level:3},{value:"Running the inference script",id:"running-the-inference-script",level:3},{value:"Running inference on Bacalhau",id:"running-inference-on-bacalhau",level:2},{value:"Prerequisite",id:"prerequisite",level:3},{value:"Structure of the command",id:"structure-of-the-command",level:3},{value:"Viewing the output",id:"viewing-the-output",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"running-inference-on-a-model-stored-on-s3",children:"Running Inference on a Model stored on S3"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://colab.research.google.com/github/bacalhau-project/examples/blob/main/model-inference/S3-Model-Inference/index.ipynb",children:(0,r.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,r.jsx)(n.a,{href:"https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=model-inference/S3-Model-Inference/index.ipynb",children:(0,r.jsx)(n.img,{src:"https://mybinder.org/badge.svg",alt:"Open In Binder"})}),"\n",(0,r.jsx)(n.a,{href:"https://github.com/bacalhau-project/bacalhau",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/github/stars/bacalhau-project/bacalhau?style=social",alt:"stars - badge-generator"})})]}),"\n",(0,r.jsx)(n.p,{children:"In this example, we will demonstrate how to run inference on a model stored on Amazon S3. We will use a PyTorch model trained on the MNIST dataset."}),"\n",(0,r.jsx)(n.h2,{id:"running-locally",children:"Running Locally"}),"\n",(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Python"}),"\n",(0,r.jsx)(n.li,{children:"PyTorch"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"downloading-the-datasets",children:"Downloading the Datasets"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"%%bash\nwget https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/model/pytorch-training-2020-11-21-22-02-56-203/model.tar.gz\nwget https://raw.githubusercontent.com/js-ts/mnist-test/main/digit.png\n"})}),"\n",(0,r.jsx)(n.h3,{id:"creating-the-inference-script",children:"Creating the Inference Script"}),"\n",(0,r.jsx)(n.p,{children:"This script is designed to load a pretrained PyTorch model for MNIST digit classification from a tar.gz file, extract it, and use the model to perform inference on a given input image."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"%%writefile inference.py\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torch.autograd import Variable\nimport argparse\nimport tarfile\n\nclass CustomModel(torch.nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n        self.conv1 = torch.nn.Conv2d(1, 10, 5)\n        self.conv2 = torch.nn.Conv2d(10, 20, 5)\n        self.fc1 = torch.nn.Linear(320, 50)\n        self.fc2 = torch.nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, 2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        output = torch.log_softmax(x, dim=1)\n        return output\n\ndef extract_tar_gz(file_path, output_dir):\n    with tarfile.open(file_path, 'r:gz') as tar:\n        tar.extractall(path=output_dir)\n\n# Parse command-line arguments\nparser = argparse.ArgumentParser()\nparser.add_argument('--tar_gz_file_path', type=str, required=True, help='Path to the tar.gz file')\nparser.add_argument('--output_directory', type=str, required=True, help='Output directory to extract the tar.gz file')\nparser.add_argument('--image_path', type=str, required=True, help='Path to the input image file')\nargs = parser.parse_args()\n\n# Extract the tar.gz file\ntar_gz_file_path = args.tar_gz_file_path\noutput_directory = args.output_directory\nextract_tar_gz(tar_gz_file_path, output_directory)\n\n# Load the model\nmodel_path = f\"{output_directory}/model.pth\"\nmodel = CustomModel()\nmodel.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\nmodel.eval()\n\n# Transformations for the MNIST dataset\ntransform = transforms.Compose([\n    transforms.Resize((28, 28)),\n    transforms.Grayscale(num_output_channels=1),\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,)),\n])\n\n# Function to run inference on an image\ndef run_inference(image, model):\n    image_tensor = transform(image).unsqueeze(0)  # Apply transformations and add batch dimension\n    input = Variable(image_tensor)\n\n    # Perform inference\n    output = model(input)\n    _, predicted = torch.max(output.data, 1)\n    return predicted.item()\n\n# Example usage\nimage_path = args.image_path\nimage = Image.open(image_path)\npredicted_class = run_inference(image, model)\nprint(f\"Predicted class: {predicted_class}\")\n\n"})}),"\n",(0,r.jsx)(n.h3,{id:"running-the-inference-script",children:"Running the inference script"}),"\n",(0,r.jsx)(n.p,{children:"To use this script, you need to provide the paths to the tar.gz file containing the pre-trained model, the output directory where the model will be extracted, and the input image file for which you want to perform inference. The script will output the predicted digit (class) for the given input image."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"%%bash\npython inference.py --tar_gz_file_path ./model.tar.gz --output_directory ./model --image_path ./digit.png\n"})}),"\n",(0,r.jsx)(n.h2,{id:"running-inference-on-bacalhau",children:"Running inference on Bacalhau"}),"\n",(0,r.jsx)(n.h3,{id:"prerequisite",children:"Prerequisite"}),"\n",(0,r.jsxs)(n.p,{children:["To get started, you need to install the Bacalhau client, see more information ",(0,r.jsx)(n.a,{href:"https://docs.bacalhau.org/getting-started/installation",children:"here"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"%%bash --out job_id\nbacalhau docker run \\\n--wait \\\n--id-only \\\n--timeout 3600 \\\n--wait-timeout-secs 3600 \\\n-w /inputs \\\n-i src=s3://sagemaker-sample-files/datasets/image/MNIST/model/pytorch-training-2020-11-21-22-02-56-203/model.tar.gz,dst=/model/,opt=region=us-east-1 \\\n-i git://github.com/js-ts/mnist-test.git \\\npytorch/pytorch \\\n -- python /inputs/js-ts/mnist-test/inference.py --tar_gz_file_path /model/model.tar.gz --output_directory /model-pth --image_path /inputs/js-ts/mnist-test/image.png\n"})}),"\n",(0,r.jsx)(n.h3,{id:"structure-of-the-command",children:"Structure of the command"}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-w /inputs"})," Setting the current working directory at /inputs in the container"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-i src=s3://sagemaker-sample-files/datasets/image/MNIST/model/pytorch-training-2020-11-21-22-02-56-203/model.tar.gz,dst=/model/,opt=region=us-east-1"}),": Mounting the s3 bucket at the destination path provided\n",(0,r.jsx)(n.code,{children:"/model/"})," and specifying the region where the bucket is located ",(0,r.jsx)(n.code,{children:"opt=region=us-east-1"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-i git://github.com/js-ts/mnist-test.git"}),": Flag to mount the source code repo from GitHub. It would mount the repo at ",(0,r.jsx)(n.code,{children:"/inputs/js-ts/mnist-test"}),"\nin this case it also contains the test image."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"pytorch/pytorch"}),": The name of the Docker image."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-- python /inputs/js-ts/mnist-test/inference.py --tar_gz_file_path /model/model.tar.gz --output_directory /model-pth --image_path /inputs/js-ts/mnist-test/image.png"}),": The command to run inference on the model."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"/model/model.tar.gz"})," is the path to the model file."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"/model-pth"})," is the output directory for the model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"/inputs/js-ts/mnist-test/image.png"})," is the path to the input image."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The job has been submitted and Bacalhau has printed out the related job id. We store that in an environment variable so that we can reuse it later on."}),"\n",(0,r.jsx)(n.h3,{id:"viewing-the-output",children:"Viewing the output"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"%%bash\nbacalhau logs ${JOB_ID}\n"})})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>o,a:()=>s});var r=t(7294);const i={},a=r.createContext(i);function s(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);