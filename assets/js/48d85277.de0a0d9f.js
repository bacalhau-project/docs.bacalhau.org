"use strict";(self.webpackChunkbacalhau_docs=self.webpackChunkbacalhau_docs||[]).push([[6796],{3905:(e,a,t)=>{t.d(a,{Zo:()=>c,kt:()=>m});var n=t(7294);function o(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){o(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function i(e,a){if(null==e)return{};var t,n,o=function(e,a){if(null==e)return{};var t,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||(o[t]=e[t]);return o}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var s=n.createContext({}),d=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},c=function(e){var a=d(e.components);return n.createElement(s.Provider,{value:a},e.children)},p={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},u=n.forwardRef((function(e,a){var t=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),u=d(t),m=o,b=u["".concat(s,".").concat(m)]||u[m]||p[m]||r;return t?n.createElement(b,l(l({ref:a},c),{},{components:t})):n.createElement(b,l({ref:a},c))}));function m(e,a){var t=arguments,o=a&&a.mdxType;if("string"==typeof e||o){var r=t.length,l=new Array(r);l[0]=u;var i={};for(var s in a)hasOwnProperty.call(a,s)&&(i[s]=a[s]);i.originalType=e,i.mdxType="string"==typeof e?e:o,l[1]=i;for(var d=2;d<r;d++)l[d]=t[d];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}u.displayName="MDXCreateElement"},2565:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>s,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var n=t(7462),o=(t(7294),t(3905));const r={sidebar_label:"Python - Pandas",sidebar_position:2},l="Running Pandas on Bacalhau",i={unversionedId:"examples/workload-onboarding/python-pandas/index",id:"examples/workload-onboarding/python-pandas/index",title:"Running Pandas on Bacalhau",description:"Open In Colab",source:"@site/docs/examples/workload-onboarding/python-pandas/index.md",sourceDirName:"examples/workload-onboarding/python-pandas",slug:"/examples/workload-onboarding/python-pandas/",permalink:"/examples/workload-onboarding/python-pandas/",draft:!1,editUrl:"https://github.com/bacalhau-project/docs.bacalhau.org/blob/main/docs/examples/workload-onboarding/python-pandas/index.md",tags:[],version:"current",lastUpdatedAt:1675457528,formattedLastUpdatedAt:"Feb 3, 2023",sidebarPosition:2,frontMatter:{sidebar_label:"Python - Pandas",sidebar_position:2},sidebar:"documentationSidebar",previous:{title:"Sparkov-Data-Generation",permalink:"/examples/workload-onboarding/Sparkov-Data-Generation/"},next:{title:"Python-Custom-Container",permalink:"/examples/workload-onboarding/Python-Custom-Container/"}},s={},d=[{value:"Introduction",id:"introduction",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"1. Getting Started with Pandas Locally",id:"1-getting-started-with-pandas-locally",level:2},{value:"Importing data from CSV to DataFrame",id:"importing-data-from-csv-to-dataframe",level:3},{value:"Running the script",id:"running-the-script",level:3},{value:"2. Running Pandas Jobs At Scale on Bacalhau",id:"2-running-pandas-jobs-at-scale-on-bacalhau",level:2},{value:"Run the Job",id:"run-the-job",level:3}],c={toc:d};function p(e){let{components:a,...t}=e;return(0,o.kt)("wrapper",(0,n.Z)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"running-pandas-on-bacalhau"},"Running Pandas on Bacalhau"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/bacalhau-project/examples/blob/main/workload-onboarding/python-pandas/index.ipynb"},(0,o.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})),"\n",(0,o.kt)("a",{parentName:"p",href:"https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=workload-onboarding/python-pandas/index.ipynb"},(0,o.kt)("img",{parentName:"a",src:"https://mybinder.org/badge.svg",alt:"Open In Binder"}))),(0,o.kt)("h3",{id:"introduction"},"Introduction"),(0,o.kt)("p",null,"Pandas is a Python package that provides fast, flexible, and expressive data structures designed to make working with data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis/manipulation tool available in any language. It is already well on its way towards this goal."),(0,o.kt)("h3",{id:"prerequisites"},"Prerequisites"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Python"),(0,o.kt)("li",{parentName:"ul"},"The Bacalhau client - ",(0,o.kt)("a",{parentName:"li",href:"https://docs.bacalhau.org/getting-started/installation"},"Installation instructions"))),(0,o.kt)("h2",{id:"1-getting-started-with-pandas-locally"},"1. Getting Started with Pandas Locally"),(0,o.kt)("p",null,"The goal of this section is to show you how to develop a script to perform a task. We will then place this script in a container and run it at scale on Bacalhau. But first, you will need to install the Pandas library from pip."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"pip install pandas\n")),(0,o.kt)("h3",{id:"importing-data-from-csv-to-dataframe"},"Importing data from CSV to DataFrame"),(0,o.kt)("p",null,"Pandas is built around the idea of a DataFrame, a container for representing data. Below you will create a DataFrame by importing a CSV file. A CSV file is a text file with one record of data per line. The values within the record are separated using the \u201ccomma\u201d character. Pandas provides a useful method, named ",(0,o.kt)("inlineCode",{parentName:"p"},"read_csv()")," to read the contents of the CSV file into a DataFrame. For example, we can create a file named ",(0,o.kt)("inlineCode",{parentName:"p"},"transactions.csv")," containing details of Transactions. The CSV file is stored in the same directory that contains Python script."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'%%writefile read_csv.py\nimport pandas as pd\n\nprint(pd.read_csv("transactions.csv"))\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"# Downloading the dataset\nwget https://cloudflare-ipfs.com/ipfs/QmfKJT13h5k1b23ja3ZCVg5nFL9oKz2bVXc8oXgtwiwhjz/transactions.csv\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"cat transactions.csv\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"hash,nonce,block_hash,block_number,transaction_index,from_address,to_address,value,gas,gas_price,input,block_timestamp,max_fee_per_gas,max_priority_fee_per_gas,transaction_type\n0x04cbcb236043d8fb7839e07bbc7f5eed692fb2ca55d897f1101eac3e3ad4fab8,12,0x246edb4b351d93c27926f4649bcf6c24366e2a7c7c718dc9158eea20c03bc6ae,483920,0,0x1b63142628311395ceafeea5667e7c9026c862ca,0xf4eced2f682ce333f96f2d8966c613ded8fc95dd,0,150853,50000000000,0xa9059cbb000000000000000000000000ac4df82fe37ea2187bc8c011a23d743b4f39019a00000000000000000000000000000000000000000000000000000000000186a0,1446561880,,,0\n0xcea6f89720cc1d2f46cc7a935463ae0b99dd5fad9c91bb7357de5421511cee49,84,0x246edb4b351d93c27926f4649bcf6c24366e2a7c7c718dc9158eea20c03bc6ae,483920,1,0x9b22a80d5c7b3374a05b446081f97d0a34079e7f,0xf4eced2f682ce333f96f2d8966c613ded8fc95dd,0,150853,50000000000,0xa9059cbb00000000000000000000000066f183060253cfbe45beff1e6e7ebbe318c81e560000000000000000000000000000000000000000000000000000000000030d40,1446561880,,,0\n0x463d53f0ad57677a3b430a007c1c31d15d62c37fab5eee598551697c297c235c,88,0x246edb4b351d93c27926f4649bcf6c24366e2a7c7c718dc9158eea20c03bc6ae,483920,2,0x9df428a91ff0f3635c8f0ce752933b9788926804,0x9e669f970ec0f49bb735f20799a7e7c4a1c274e2,11000440000000000,90000,50000000000,0x,1446561880,,,0\n0x05287a561f218418892ab053adfb3d919860988b19458c570c5c30f51c146f02,20085,0x246edb4b351d93c27926f4649bcf6c24366e2a7c7c718dc9158eea20c03bc6ae,483920,3,0x2a65aca4d5fc5b5c859090a6c34d164135398226,0x743b8aeedc163c0e3a0fe9f3910d146c48e70da8,1530219620000000000,90000,50000000000,0x,1446561880,,,0\n")),(0,o.kt)("h3",{id:"running-the-script"},"Running the script"),(0,o.kt)("p",null,"Now let's run the script to read in the CSV file. The output will be a DataFrame object."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"python3 read_csv.py\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"                                                hash  ...  transaction_type\n0  0x04cbcb236043d8fb7839e07bbc7f5eed692fb2ca55d8...  ...                 0\n1  0xcea6f89720cc1d2f46cc7a935463ae0b99dd5fad9c91...  ...                 0\n2  0x463d53f0ad57677a3b430a007c1c31d15d62c37fab5e...  ...                 0\n3  0x05287a561f218418892ab053adfb3d919860988b1945...  ...                 0\n\n[4 rows x 15 columns]\n")),(0,o.kt)("h2",{id:"2-running-pandas-jobs-at-scale-on-bacalhau"},"2. Running Pandas Jobs At Scale on Bacalhau"),(0,o.kt)("p",null,"To run pandas on Bacalhau you must store your assets in a location that Bacalhau has access to. We usually default to storing data on IPFS and code in a container, but you can also easily upload your script to IPFS too."),(0,o.kt)("p",null,"If you are interested in finding out more about how to ingest your data into IPFS, please see the ",(0,o.kt)("a",{parentName:"p",href:"/examples/data-ingestion/"},"data ingestion guide"),"."),(0,o.kt)("p",null,"We've already uploaded the script and data to IPFS to the following CID: ",(0,o.kt)("inlineCode",{parentName:"p"},"QmfKJT13h5k1b23ja3ZCVg5nFL9oKz2bVXc8oXgtwiwhjz"),". You can look at this by browsing to one of the HTTP IPFS proxies like ",(0,o.kt)("a",{parentName:"p",href:"https://cloudflare-ipfs.com/ipfs/QmfKJT13h5k1b23ja3ZCVg5nFL9oKz2bVXc8oXgtwiwhjz/"},"ipfs.io")," or ",(0,o.kt)("a",{parentName:"p",href:"https://bafybeih4hyydvojazlyv5zseelgn5u67iq2wbrbk2q4xoiw2d3cacdmzlu.ipfs.w3s.link/"},"w3s.link"),"."),(0,o.kt)("h3",{id:"run-the-job"},"Run the Job"),(0,o.kt)("p",null,"Now we're ready to run a Bacalhau job, whilst mounting the Pandas script and data from IPFS. We'll use the ",(0,o.kt)("inlineCode",{parentName:"p"},"bacalhau docker run")," command to do this. The ",(0,o.kt)("inlineCode",{parentName:"p"},"-v")," flag allows us to mount a file or directory from IPFS into the container. The ",(0,o.kt)("inlineCode",{parentName:"p"},"-v")," flag takes two arguments, the first is the IPFS CID and the second is the path to the directory in the container. The ",(0,o.kt)("inlineCode",{parentName:"p"},"-v")," flag can be used multiple times to mount multiple directories."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"}," bacalhau docker run \\\n--wait \\\n--id-only \\\n-v QmfKJT13h5k1b23ja3ZCVg5nFL9oKz2bVXc8oXgtwiwhjz:/files \\\n-w /files \\\namancevice/pandas \\\n-- python read_csv.py\n")),(0,o.kt)("p",null,"Running the commands will output a UUID (like ",(0,o.kt)("inlineCode",{parentName:"p"},"e6377c99-b637-4661-a334-6ce98fcf037c"),"). This is the ID of the job that was created. You can check the status of the job with the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau list --id-filter ${JOB_ID}\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"\x1b[92;100m CREATED  \x1b[0m\x1b[92;100m ID       \x1b[0m\x1b[92;100m JOB                     \x1b[0m\x1b[92;100m STATE     \x1b[0m\x1b[92;100m VERIFIED \x1b[0m\x1b[92;100m PUBLISHED               \x1b[0m\n\x1b[97;40m 13:38:11 \x1b[0m\x1b[97;40m d48079d4 \x1b[0m\x1b[97;40m Docker amancevice/pa... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmY2MEETWyX77B... \x1b[0m\n")),(0,o.kt)("p",null,'Where it says "',(0,o.kt)("inlineCode",{parentName:"p"},"Published"),'", that means the job is done, and we can get the results.'),(0,o.kt)("p",null,"If there is an error you can view the error using the following command bacalhau describe"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau describe ${JOB_ID}\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'APIVersion: V1beta1\nClientID: 77cf46c04f88ffb1c3e0e4b6e443724e8d2d87074d088ef1a6294a448fa85d2e\nCreatedAt: "2022-11-23T13:38:11.136995358Z"\nDeal:\n  Concurrency: 1\nExecutionPlan:\n  ShardsTotal: 1\nID: d48079d4-1358-4ce1-8a9e-5b9e6ae40bda\nJobState:\n  Nodes:\n    QmSyJ8VUd4YSPwZFJSJsHmmmmg7sd4BAc2yHY73nisJo86:\n      Shards:\n        "0":\n          NodeId: QmSyJ8VUd4YSPwZFJSJsHmmmmg7sd4BAc2yHY73nisJo86\n          PublishedResults: {}\n          State: Cancelled\n          VerificationResult: {}\n    QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF:\n      Shards:\n        "0":\n          NodeId: QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF\n          PublishedResults:\n            CID: QmY2MEETWyX77BBYBNBpUW5bjkVAyP87EotPDVW2vjHG8K\n            Name: job-d48079d4-1358-4ce1-8a9e-5b9e6ae40bda-shard-0-host-QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF\n            StorageSource: IPFS\n          RunOutput:\n            exitCode: 0\n            runnerError: ""\n            stderr: ""\n            stderrtruncated: false\n            stdout: |2\n                                                              hash  ...  transaction_type\n              0  0x04cbcb236043d8fb7839e07bbc7f5eed692fb2ca55d8...  ...                 0\n              1  0xcea6f89720cc1d2f46cc7a935463ae0b99dd5fad9c91...  ...                 0\n              2  0x463d53f0ad57677a3b430a007c1c31d15d62c37fab5e...  ...                 0\n              3  0x05287a561f218418892ab053adfb3d919860988b1945...  ...                 0\n\n              [4 rows x 15 columns]\n            stdouttruncated: false\n          State: Completed\n          Status: \'Got results proposal of length: 0\'\n          VerificationResult:\n            Complete: true\n            Result: true\nRequesterNodeID: QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF\nRequesterPublicKey: CAASpgIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCehDIWl72XKJi1tsrYM9JjAWt3n6hNzrCA+IVRXixK1sJVTLMpsxEP8UKJI+koAWkAUuY8yi6DMzot0owK4VpM3PYp34HdKi2hTjzM8pjCVb70XVXt6k9bzj4KmbiQTuEkQfvwIRmgxb2jrkRdTpZmhMb1Q7StR/nrGa/bx75Vpupx1EYH6+LixYnnV5WbCUK/kjpBW8SF5v+f9ZO61KHd9DMpdhJnzocTGq17tAjHh3birke0xlP98JjxlMkzzvIAuFsnH0zBIgjmHDA1Yi5DcOPWgE0jUfGlSDC1t2xITVoofHQcXDjkHZE6OhxswNYPd7cnTf9OppLddFdQnga5AgMBAAE=\nSpec:\n  Docker:\n    Entrypoint:\n    - python\n    - read_csv.py\n    Image: amancevice/pandas\n    WorkingDirectory: /files\n  Engine: Docker\n  Language:\n    JobContext: {}\n  Publisher: Estuary\n  Resources:\n    GPU: ""\n  Sharding:\n    BatchSize: 1\n    GlobPatternBasePath: /inputs\n  Timeout: 1800\n  Verifier: Noop\n  Wasm: {}\n  inputs:\n  - CID: QmfKJT13h5k1b23ja3ZCVg5nFL9oKz2bVXc8oXgtwiwhjz\n    StorageSource: IPFS\n    path: /files\n  outputs:\n  - Name: outputs\n    StorageSource: IPFS\n    path: /outputs\n')),(0,o.kt)("p",null,"The describe command will display the logs and error messages from your job. There's no errors this time (lucky?) so now let's create a temporary directory to save our results."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"rm -rf results && mkdir -p results\n")),(0,o.kt)("p",null,"To Download the results of your job, run the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau get ${JOB_ID}  --output-dir results\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Fetching results of job 'd48079d4-1358-4ce1-8a9e-5b9e6ae40bda'...\nResults for job 'd48079d4-1358-4ce1-8a9e-5b9e6ae40bda' have been written to...\nresults\n")),(0,o.kt)("p",null,"After the download has finished you should\nsee the following contents in pandas-results directory"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"ls results/combined_results/\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"outputs\nstderr\nstdout\n")),(0,o.kt)("p",null,"The structure of the files and directories will look like this:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},".\n\u251c\u2500\u2500 combined_results\n\u2502\xa0\xa0 \u251c\u2500\u2500 outputs\n\u2502\xa0\xa0 \u251c\u2500\u2500 stderr\n\u2502\xa0\xa0 \u2514\u2500\u2500 stdout\n\u251c\u2500\u2500 per_shard\n\u2502\xa0\xa0 \u2514\u2500\u2500 0_node_QmSyJ8VU\n\u2502\xa0\xa0     \u251c\u2500\u2500 exitCode\n\u2502\xa0\xa0     \u251c\u2500\u2500 outputs\n\u2502\xa0\xa0     \u251c\u2500\u2500 stderr\n\u2502\xa0\xa0     \u2514\u2500\u2500 stdout\n\u2514\u2500\u2500 raw\n    \u2514\u2500\u2500 QmY2MEETWyX77BBYBNBpUW5bjkVAyP87EotPDVW2vjHG8K\n        \u251c\u2500\u2500 exitCode\n        \u251c\u2500\u2500 outputs\n        \u251c\u2500\u2500 stderr\n        \u2514\u2500\u2500 stdout\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"stdout")," contains things printed to the console like outputs, etc.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"stderr")," contains any errors. In this case, since there are no errors, it's will be empty")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"p"},"outputs")," folder is the volume you named when you started the job with the ",(0,o.kt)("inlineCode",{parentName:"p"},"-o")," flag. In addition, you will always have a ",(0,o.kt)("inlineCode",{parentName:"p"},"outputs")," volume, which is provided by default."))),(0,o.kt)("p",null,"Because your script is printed to stdout, the output will appear in the stdout file. You can read this by typing the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"cat results/combined_results/stdout\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"                                                hash  ...  transaction_type\n0  0x04cbcb236043d8fb7839e07bbc7f5eed692fb2ca55d8...  ...                 0\n1  0xcea6f89720cc1d2f46cc7a935463ae0b99dd5fad9c91...  ...                 0\n2  0x463d53f0ad57677a3b430a007c1c31d15d62c37fab5e...  ...                 0\n3  0x05287a561f218418892ab053adfb3d919860988b1945...  ...                 0\n\n[4 rows x 15 columns]\n")),(0,o.kt)("p",null,"Success! The next step is to scale up your data and your processing via multiple jobs or sharding. You might be interested in looking at:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/examples/data-engineering/blockchain-etl/"},'An example running hundreds of jobs over "big data"')),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"/examples/data-engineering/simple-parallel-workloads/"},"A simple sharding example"))))}p.isMDXComponent=!0}}]);