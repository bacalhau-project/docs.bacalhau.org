"use strict";(self.webpackChunkbacalhau_docs=self.webpackChunkbacalhau_docs||[]).push([[4013],{4008:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>h});var s=n(5893),a=n(1151);const i={},o="Scripting Bacalhau with Python",r={id:"examples/workload-onboarding/python-script/index",title:"Scripting Bacalhau with Python",description:"stars - badge-generator",source:"@site/docs/examples/workload-onboarding/python-script/index.md",sourceDirName:"examples/workload-onboarding/python-script",slug:"/examples/workload-onboarding/python-script/",permalink:"/examples/workload-onboarding/python-script/",draft:!1,unlisted:!1,editUrl:"https://github.com/bacalhau-project/docs.bacalhau.org/blob/main/docs/examples/workload-onboarding/python-script/index.md",tags:[],version:"current",lastUpdatedAt:1702252086,formattedLastUpdatedAt:"Dec 10, 2023",frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Python File",permalink:"/examples/workload-onboarding/trivial-python/"},next:{title:"Data Engineering",permalink:"/category/data-engineering"}},l={},h=[{value:"TD;LR",id:"tdlr",level:2},{value:"Prerequisite",id:"prerequisite",level:2},{value:"Executing Bacalhau Jobs with Python Scripts",id:"executing-bacalhau-jobs-with-python-scripts",level:2},{value:"Next Steps",id:"next-steps",level:3}];function c(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h1,{id:"scripting-bacalhau-with-python",children:"Scripting Bacalhau with Python"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://github.com/bacalhau-project/bacalhau",children:(0,s.jsx)(t.img,{src:"https://img.shields.io/github/stars/bacalhau-project/bacalhau?style=social",alt:"stars - badge-generator"})})}),"\n",(0,s.jsx)(t.p,{children:"Bacalhau allows you to easily execute batch jobs via the CLI. But sometimes you need to do more than that. You might need to execute a script that requires user input, or you might need to execute a script that requires a lot of parameters. In any case, you probably want to execute your jobs in a repeatable manner."}),"\n",(0,s.jsx)(t.p,{children:"This example demonstrates a simple Python script that is able to orchestrate the execution of lots of jobs in a repeatable manner."}),"\n",(0,s.jsx)(t.h2,{id:"tdlr",children:"TD;LR"}),"\n",(0,s.jsx)(t.p,{children:"Running Python script in Bacalhau"}),"\n",(0,s.jsx)(t.h2,{id:"prerequisite",children:"Prerequisite"}),"\n",(0,s.jsxs)(t.p,{children:["To get started, you need to install the Bacalhau client, see more information ",(0,s.jsx)(t.a,{href:"https://docs.bacalhau.org/getting-started/installation",children:"here"})]}),"\n",(0,s.jsx)(t.h2,{id:"executing-bacalhau-jobs-with-python-scripts",children:"Executing Bacalhau Jobs with Python Scripts"}),"\n",(0,s.jsxs)(t.p,{children:["To demonstrate this example, I will use the data generated from the ",(0,s.jsx)(t.a,{href:"/examples/data-engineering/blockchain-etl/",children:"ethereum analysis example"}),". This produced a list of hashes that I will iterate over and execute a job for each one."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"%%writefile hashes.txt\nbafybeihvtzberlxrsz4lvzrzvpbanujmab3hr5okhxtbgv2zvonqos2l3i\nbafybeifb25fgxrzu45lsc47gldttomycqcsao22xa2gtk2ijbsa5muzegq\nbafybeig4wwwhs63ly6wbehwd7tydjjtnw425yvi2tlzt3aii3pfcj6hvoq\nbafybeievpb5q372q3w5fsezflij3wlpx6thdliz5xowimunoqushn3cwka\nbafybeih6te26iwf5kzzby2wqp67m7a5pmwilwzaciii3zipvhy64utikre\nbafybeicjd4545xph6rcyoc74wvzxyaz2vftapap64iqsp5ky6nz3f5yndm\n"})}),"\n",(0,s.jsxs)(t.p,{children:["Now let's run the following script. You can execute this script anywhere with ",(0,s.jsx)(t.code,{children:"python bacalhau.py"}),"."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'%%writefile bacalhau.py\nimport json, glob, os, multiprocessing, shutil, subprocess, tempfile, time\n\n# checkStatusOfJob checks the status of a Bacalhau job\ndef checkStatusOfJob(job_id: str) -> str:\n    assert len(job_id) > 0\n    p = subprocess.run(\n        ["bacalhau", "list", "--output", "json", "--id-filter", job_id],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n    )\n    r = parseJobStatus(p.stdout)\n    if r == "":\n        print("job status is empty! %s" % job_id)\n    elif r == "Completed":\n        print("job completed: %s" % job_id)\n    else:\n        print("job not completed: %s - %s" % (job_id, r))\n\n    return r\n\n\n# submitJob submits a job to the Bacalhau network\ndef submitJob(cid: str) -> str:\n    assert len(cid) > 0\n    p = subprocess.run(\n        [\n            "bacalhau",\n            "docker",\n            "run",\n            "--id-only",\n            "--wait=false",\n            "--input",\n            "ipfs://" + cid + ":/inputs/data.tar.gz",\n            "ghcr.io/bacalhau-project/examples/blockchain-etl:0.0.6",\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n    )\n    if p.returncode != 0:\n        print("failed (%d) job: %s" % (p.returncode, p.stdout))\n    job_id = p.stdout.strip()\n    print("job submitted: %s" % job_id)\n\n    return job_id\n\n\n# getResultsFromJob gets the results from a Bacalhau job\ndef getResultsFromJob(job_id: str) -> str:\n    assert len(job_id) > 0\n    temp_dir = tempfile.mkdtemp()\n    print("getting results for job: %s" % job_id)\n    for i in range(0, 5): # try 5 times\n        p = subprocess.run(\n            [\n                "bacalhau",\n                "get",\n                "--output-dir",\n                temp_dir,\n                job_id,\n            ],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n        if p.returncode == 0:\n            break\n        else:\n            print("failed (exit %d) to get job: %s" % (p.returncode, p.stdout))\n\n    return temp_dir\n\n\n# parseJobStatus parses the status of a Bacalhau job\ndef parseJobStatus(result: str) -> str:\n    if len(result) == 0:\n        return ""\n    r = json.loads(result)\n    if len(r) > 0:\n        return r[0]["State"]["State"]\n    return ""\n\n\n# parseHashes splits lines from a text file into a list\ndef parseHashes(filename: str) -> list:\n    assert os.path.exists(filename)\n    with open(filename, "r") as f:\n        hashes = f.read().splitlines()\n    return hashes\n\n\ndef main(file: str, num_files: int = -1):\n    # Use multiprocessing to work in parallel\n    count = multiprocessing.cpu_count()\n    with multiprocessing.Pool(processes=count) as pool:\n        hashes = parseHashes(file)[:num_files]\n        print("submitting %d jobs" % len(hashes))\n        job_ids = pool.map(submitJob, hashes)\n        assert len(job_ids) == len(hashes)\n\n        print("waiting for jobs to complete...")\n        while True:\n            job_statuses = pool.map(checkStatusOfJob, job_ids)\n            total_finished = sum(map(lambda x: x == "Completed", job_statuses))\n            if total_finished >= len(job_ids):\n                break\n            print("%d/%d jobs completed" % (total_finished, len(job_ids)))\n            time.sleep(2)\n\n        print("all jobs completed, saving results...")\n        results = pool.map(getResultsFromJob, job_ids)\n        print("finished saving results")\n\n        # Do something with the results\n        shutil.rmtree("results", ignore_errors=True)\n        os.makedirs("results", exist_ok=True)\n        for r in results:\n            path = os.path.join(r, "outputs", "*.csv")\n            csv_file = glob.glob(path)\n            for f in csv_file:\n                print("moving %s to results" % f)\n                shutil.move(f, "results")\n\nif __name__ == "__main__":\n    main("hashes.txt", 10)\n\n'})}),"\n",(0,s.jsx)(t.p,{children:"This code has a few interesting features:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Change the value in the ",(0,s.jsx)(t.code,{children:"main"})," call to change the number of jobs to execute"]}),"\n",(0,s.jsx)(t.li,{children:"Because all jobs are complete at different times, there's a loop to check that all jobs have been completed before downloading the results -- if you don't do this you'll likely see an error when trying to download the results"}),"\n",(0,s.jsx)(t.li,{children:"When downloading the results, the IPFS get often times out, so I wrapped that in a loop"}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Let's run it!"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"%%bash\npython bacalhau.py\n"})}),"\n",(0,s.jsx)(t.p,{children:"Hopefully, the results directory contains all the combined results from the jobs we just executed. Here's we're expecting to see CSV files:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"%%bash\nls -l results\n"})}),"\n",(0,s.jsx)(t.p,{children:"Success! We've now executed a bunch of jobs in parallel using Python. This is a great way to execute lots of jobs in a repeatable manner. You can alter the file above for your purposes."}),"\n",(0,s.jsx)(t.h3,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(t.p,{children:"You might also be interested in the following examples:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/examples/data-engineering/blockchain-etl/",children:"Analysing Ethereum Data with Python"})}),"\n"]})]})}function u(e={}){const{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},1151:(e,t,n)=>{n.d(t,{Z:()=>r,a:()=>o});var s=n(7294);const a={},i=s.createContext(a);function o(e){const t=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(i.Provider,{value:t},e.children)}}}]);