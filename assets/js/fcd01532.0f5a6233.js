"use strict";(self.webpackChunkbacalhau_docs=self.webpackChunkbacalhau_docs||[]).push([[6084],{256:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>s});var r=i(5893),t=i(1151);const o={sidebar_label:"Huggingface-Model-Inference",sidebar_position:1},l="Running Inference on Dolly 2.0 Model with Hugging Face",a={id:"examples/model-inference/Huggingface-Model-Inference/index",title:"Running Inference on Dolly 2.0 Model with Hugging Face",description:"Open In Colab",source:"@site/docs/examples/model-inference/Huggingface-Model-Inference/index.md",sourceDirName:"examples/model-inference/Huggingface-Model-Inference",slug:"/examples/model-inference/Huggingface-Model-Inference/",permalink:"/examples/model-inference/Huggingface-Model-Inference/",draft:!1,unlisted:!1,editUrl:"https://github.com/bacalhau-project/docs.bacalhau.org/blob/main/docs/examples/model-inference/Huggingface-Model-Inference/index.md",tags:[],version:"current",lastUpdatedAt:1702309452,formattedLastUpdatedAt:"Dec 11, 2023",sidebarPosition:1,frontMatter:{sidebar_label:"Huggingface-Model-Inference",sidebar_position:1},sidebar:"documentationSidebar",previous:{title:"Model Inference",permalink:"/category/model-inference"},next:{title:"Object Detection - YOLOv5",permalink:"/examples/model-inference/object-detection-yolo5/"}},c={},s=[{value:"Introduction",id:"introduction",level:2},{value:"Running locally",id:"running-locally",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Installing dependencies",id:"installing-dependencies",level:3},{value:"Building the container (optional)",id:"building-the-container-optional",level:2},{value:"Prerequisite",id:"prerequisite",level:3},{value:"Running Inference on Bacalhau",id:"running-inference-on-bacalhau",level:2},{value:"Prerequisite",id:"prerequisite-1",level:3},{value:"Structure of the command",id:"structure-of-the-command",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"running-inference-on-dolly-20-model-with-hugging-face",children:"Running Inference on Dolly 2.0 Model with Hugging Face"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://colab.research.google.com/github/bacalhau-project/examples/blob/main/model-inference/Huggingface-Model-Inference/index.ipynb",children:(0,r.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,r.jsx)(n.a,{href:"https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=model-inference/Huggingface-Model-Inference/index.ipynb",children:(0,r.jsx)(n.img,{src:"https://mybinder.org/badge.svg",alt:"Open In Binder"})}),"\n",(0,r.jsx)(n.a,{href:"https://github.com/bacalhau-project/bacalhau",children:(0,r.jsx)(n.img,{src:"https://img.shields.io/github/stars/bacalhau-project/bacalhau?style=social",alt:"stars - badge-generator"})})]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"Dolly 2.0, the groundbreaking, open-source, instruction-following Large Language Model (LLM) that has been fine-tuned on a human-generated instruction dataset, licensed for both research and commercial purposes. Developed using the EleutherAI Pythia model family, this 12-billion-parameter language model is built exclusively on a high-quality, human-generated instruction following dataset, contributed by Databricks employees."}),"\n",(0,r.jsx)(n.p,{children:"Dolly 2.0 package is open source, including the training code, dataset, and model weights, all available for commercial use. This unprecedented move empowers organizations to create, own, and customize robust LLMs capable of engaging in human-like interactions, without the need for API access fees or sharing data with third parties."}),"\n",(0,r.jsx)(n.h2,{id:"running-locally",children:"Running locally"}),"\n",(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A NVIDIA GPU"}),"\n",(0,r.jsx)(n.li,{children:"Python"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"installing-dependencies",children:"Installing dependencies"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"%%bash\npip -q install git+https://github.com/huggingface/transformers # need to install from github\npip -q install accelerate>=0.12.0\n"})}),"\n",(0,r.jsx)(n.p,{children:"Creating the inference script"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'%%writefile inference.py\nimport argparse\nimport torch\nfrom transformers import pipeline\n\ndef main(prompt_string, model_version):\n\n    # use dolly-v2-12b if you\'re using Colab Pro+, using pythia-2.8b for Free Colab\n    generate_text = pipeline(model=model_version, \n                            torch_dtype=torch.bfloat16, \n                            trust_remote_code=True,\n                            device_map="auto")\n\n    print(generate_text(prompt_string))\n\nif __name__ == "__main__":\n    parser = argparse.ArgumentParser()\n    parser.add_argument("--prompt", type=str, required=True, help="The prompt to be used in the GPT model")\n    parser.add_argument("--model_version", type=str, default="./databricks/dolly-v2-12b", help="The model version to be used")\n    args = parser.parse_args()\n    main(args.prompt, args.model_version)\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"building-the-container-optional",children:"Building the container (optional)"}),"\n",(0,r.jsx)(n.h3,{id:"prerequisite",children:"Prerequisite"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Install Docker on your local machine."}),"\n",(0,r.jsx)(n.li,{children:"Sign up for a DockerHub account if you don't already have one.\nSteps"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Step 1: Create a Dockerfile\nCreate a new file named Dockerfile in your project directory with the following content:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-Dockerfile",children:"FROM huggingface/transformers-pytorch-deepspeed-nightly-gpu\n\nRUN apt-get update -y\n\nRUN pip -q install git+https://github.com/huggingface/transformers\n\nRUN pip -q install accelerate>=0.12.0 \n\nWORKDIR /\n\n# COPY ./dolly_inference.py .\n"})}),"\n",(0,r.jsx)(n.p,{children:"This Dockerfile sets up a container with the necessary dependencies and installs the Segment Anything Model from its GitHub repository."}),"\n",(0,r.jsx)(n.p,{children:"Step 2: Build the Docker Image\nIn your terminal, navigate to the directory containing the Dockerfile and run the following command to build the Docker image:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker build -t your-dockerhub-username/sam:lite .\n"})}),"\n",(0,r.jsx)(n.p,{children:'Replace your-dockerhub-username with your actual DockerHub username. This command will build the Docker image and tag it with your DockerHub username and the name "sam".'}),"\n",(0,r.jsx)(n.p,{children:"Step 3: Push the Docker Image to DockerHub\nOnce the build process is complete, Next, push the Docker image to DockerHub using the following command:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker push your-dockerhub-username/sam:lite\n"})}),"\n",(0,r.jsx)(n.p,{children:"Again, replace your-dockerhub-username with your actual DockerHub username. This command will push the Docker image to your DockerHub repository."}),"\n",(0,r.jsx)(n.h2,{id:"running-inference-on-bacalhau",children:"Running Inference on Bacalhau"}),"\n",(0,r.jsx)(n.h3,{id:"prerequisite-1",children:"Prerequisite"}),"\n",(0,r.jsxs)(n.p,{children:["To get started, you need to install the Bacalhau client, see more information ",(0,r.jsx)(n.a,{href:"https://docs.bacalhau.org/getting-started/installation",children:"here"})]}),"\n",(0,r.jsx)(n.h3,{id:"structure-of-the-command",children:"Structure of the command"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'bacalhau docker run \\\n--gpu 1 \\\n-w /inputs \\\n-i gitlfs://huggingface.co/databricks/dolly-v2-3b.git \\\n-i https://gist.githubusercontent.com/js-ts/d35e2caa98b1c9a8f176b0b877e0c892/raw/3f020a6e789ceef0274c28fc522ebf91059a09a9/inference.py \\\njsacex/dolly_inference:latest \\\n -- python inference.py --prompt "Where is Earth located ?" --model_version "./databricks/dolly-v2-3b"\n'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"docker run"}),": Docker command to run a container from a specified image."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"--gpu 1"}),": Flag to specify the number of GPUs to use for the execution. In this case, 1 GPU will be used."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-w /inputs"}),": Flag to set the working directory inside the container to ",(0,r.jsx)(n.code,{children:"/inputs"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-i gitlfs://huggingface.co/databricks/dolly-v2-3b.git"}),": Flag to clone the Dolly V2-3B model from Hugging Face's repository using Git LFS. The files will be mounted to ",(0,r.jsx)(n.code,{children:"/inputs/databricks/dolly-v2-3b"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"-i https://gist.githubusercontent.com/js-ts/d35e2caa98b1c9a8f176b0b877e0c892/raw/3f020a6e789ceef0274c28fc522ebf91059a09a9/inference.py"}),": Flag to download the ",(0,r.jsx)(n.code,{children:"inference.py"})," script from the provided URL. The file will be mounted to ",(0,r.jsx)(n.code,{children:"/inputs/inference.py"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"jsacex/dolly_inference:latest"}),": The name and the tag of the Docker image."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["The command to run inference on the model: ",(0,r.jsx)(n.code,{children:'python inference.py --prompt "Where is Earth located ?" --model_version "./databricks/dolly-v2-3b"'}),"."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"inference.py"}),": The Python script that runs the inference process using the Dolly V2-3B model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:'--prompt "Where is Earth located ?"'}),": Specifies the text prompt to be used for the inference."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:'--model_version "./databricks/dolly-v2-3b"'}),": Specifies the path to the Dolly V2-3B model. In this case, the model files are mounted to ",(0,r.jsx)(n.code,{children:"/inputs/databricks/dolly-v2-3b"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},1151:(e,n,i)=>{i.d(n,{Z:()=>a,a:()=>l});var r=i(7294);const t={},o=r.createContext(t);function l(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);