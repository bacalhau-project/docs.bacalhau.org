"use strict";(self.webpackChunkbacalhau_docs=self.webpackChunkbacalhau_docs||[]).push([[2706],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>f});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},s=Object.keys(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)n=s[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,s=e.originalType,l=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),c=p(n),m=i,f=c["".concat(l,".").concat(m)]||c[m]||d[m]||s;return n?a.createElement(f,r(r({ref:t},u),{},{components:n})):a.createElement(f,r({ref:t},u))}));function f(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var s=n.length,r=new Array(s);r[0]=m;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o[c]="string"==typeof e?e:i,r[1]=o;for(var p=2;p<s;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},7295:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>p});var a=n(7462),i=(n(7294),n(3905));const s={sidebar_label:"Stable Diffusion - CPU",sidebar_position:1},r="Stable Diffusion on a CPU",o={unversionedId:"examples/model-inference/stable-diffusion-cpu/index",id:"examples/model-inference/stable-diffusion-cpu/index",title:"Stable Diffusion on a CPU",description:"Open In Colab",source:"@site/docs/examples/model-inference/stable-diffusion-cpu/index.md",sourceDirName:"examples/model-inference/stable-diffusion-cpu",slug:"/examples/model-inference/stable-diffusion-cpu/",permalink:"/examples/model-inference/stable-diffusion-cpu/",draft:!1,editUrl:"https://github.com/bacalhau-project/docs.bacalhau.org/blob/main/docs/examples/model-inference/stable-diffusion-cpu/index.md",tags:[],version:"current",lastUpdatedAt:1679107987,formattedLastUpdatedAt:"Mar 18, 2023",sidebarPosition:1,frontMatter:{sidebar_label:"Stable Diffusion - CPU",sidebar_position:1},sidebar:"documentationSidebar",previous:{title:"Stable-Diffusion-CKPT-Inference",permalink:"/examples/model-inference/Stable-Diffusion-CKPT-Inference/"},next:{title:"Stable Diffusion - GPU",permalink:"/examples/model-inference/stable-diffusion-gpu/"}},l={},p=[{value:"TL;DR",id:"tldr",level:3},{value:"1. Development",id:"1-development",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Converting Stable Diffusion to a CPU Model Using OpenVINO",id:"converting-stable-diffusion-to-a-cpu-model-using-openvino",level:3},{value:"Install Dependencies",id:"install-dependencies",level:3},{value:"Clone the Repository and Dependencies",id:"clone-the-repository-and-dependencies",level:3},{value:"Generating an Image",id:"generating-an-image",level:3},{value:"2. Running Stable Diffusion (CPU) on Bacalhau",id:"2-running-stable-diffusion-cpu-on-bacalhau",level:2},{value:"Prerequisites",id:"prerequisites-1",level:3},{value:"Generating an Image Using Stable Diffusion on Bacalhau",id:"generating-an-image-using-stable-diffusion-on-bacalhau",level:3}],u={toc:p},c="wrapper";function d(e){let{components:t,...s}=e;return(0,i.kt)(c,(0,a.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"stable-diffusion-on-a-cpu"},"Stable Diffusion on a CPU"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/bacalhau-project/examples/blob/main/model-inference/stable-diffusion-cpu/index.ipynb"},(0,i.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})),"\n",(0,i.kt)("a",{parentName:"p",href:"https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=model-inference/stable-diffusion/index.ipynb"},(0,i.kt)("img",{parentName:"a",src:"https://mybinder.org/badge.svg",alt:"Open In Binder"}))),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/CompVis/stable-diffusion"},"Stable Diffusion")," is a state of the art text-to-image model that generates images from text and was developed as an open source alternative to ",(0,i.kt)("a",{parentName:"p",href:"https://openai.com/dall-e-2/"},"DALL\xb7E 2"),". It is based on a ",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2102.09672"},"Diffusion Probabilistic Model")," and uses a ",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1706.03762"},"Transformer")," to generate images from text."),(0,i.kt)("p",null,"This example demonstrates how to use stable diffusion on a CPU and run it on the ",(0,i.kt)("a",{parentName:"p",href:"https://www.bacalhau.org/"},"Bacalhau")," network. The first section describes the development of the code and the container. The section section demonstrates how to run the job using ",(0,i.kt)("a",{parentName:"p",href:"https://www.bacalhau.org/"},"Bacalhau"),"."),(0,i.kt)("p",null,"The following image is an example generated by this model."),(0,i.kt)("h3",{id:"tldr"},"TL;DR"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'bacalhau docker run ghcr.io/bacalhau-project/examples/stable-diffusion-cpu:0.0.1 -- python demo.py --prompt "cod in space" --output ../outputs/cod.png\n')),(0,i.kt)("p",null,(0,i.kt)("img",{src:n(1121).Z,width:"512",height:"512"})),(0,i.kt)("h2",{id:"1-development"},"1. Development"),(0,i.kt)("p",null,"The ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/CompVis/stable-diffusion"},"original")," text-to-image stable diffusion model was trained on a fleet of GPU machines, at great cost. To use this trained model for inference, you also need to run it on a GPU."),(0,i.kt)("p",null,"However, this isn't always desired or possible. One alternative is to use a project called ",(0,i.kt)("a",{parentName:"p",href:"https://docs.openvino.ai/latest/index.html"},"OpenVINO")," from Intel that allows you to convert and optimise models from a variety of frameworks (and ONNX if your framework isn't directly supported) to run on a ",(0,i.kt)("a",{parentName:"p",href:"https://docs.openvino.ai/latest/openvino_docs_OV_UG_Working_with_devices.html"},"supported")," Intel CPU. This is what we will do in this example."),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"Heads up! This example takes about 10 minutes to generate an image on an average CPU. Whilst this demonstrates it is possible, it might not be practical.")),(0,i.kt)("h3",{id:"prerequisites"},"Prerequisites"),(0,i.kt)("p",null,"In order to run this example you need:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"A Debian-flavoured Linux (although you ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/bfirsh/stable-diffusion/tree/apple-silicon-mps-support"},"might be able")," to get it working on M1 macs)"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://docs.docker.com/get-docker/"},"Docker"))),(0,i.kt)("h3",{id:"converting-stable-diffusion-to-a-cpu-model-using-openvino"},"Converting Stable Diffusion to a CPU Model Using OpenVINO"),(0,i.kt)("p",null,"The first step is to convert the trained stable diffusion models so that they work efficiently on a CPU using OpenVINO. The example is quite complex, so we have created a ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/js-ts/stable_diffusion.openvino"},"separate repository")," (which is a fork from Github user ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/bes-dev/stable_diffusion.openvino"},"Sergei Belousov"),") to host the code. In summary, the code:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Downloads a ",(0,i.kt)("a",{parentName:"li",href:"https://huggingface.co/bes-dev/stable-diffusion-v1-4-openvino"},"pre-optimized OpenVINO version")," of ..."),(0,i.kt)("li",{parentName:"ul"},"the ",(0,i.kt)("a",{parentName:"li",href:"https://huggingface.co/CompVis/stable-diffusion-v1-4"},"original")," pre-trained stable diffusion model ..."),(0,i.kt)("li",{parentName:"ul"},"which also leverages OpenAI's ",(0,i.kt)("a",{parentName:"li",href:"https://huggingface.co/openai/clip-vit-large-patch14"},"CLIP transformer")," ..."),(0,i.kt)("li",{parentName:"ul"},"and is then wrapped inside an OpenVINO runtime, which reads in and executes the model.")),(0,i.kt)("p",null,"The core code representing these tasks can be found ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/js-ts/stable_diffusion.openvino/blob/master/stable_diffusion_engine.py"},"in the ",(0,i.kt)("inlineCode",{parentName:"a"},"stable_diffusion_engine.py")," file"),". This is a mashup that creates a pipeline necessary to tokenize the text and run the stable diffusion model. This boilerplate could be simplified by leveraging the more recent version of the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/huggingface/diffusers"},"diffusers library"),". But let's crack on."),(0,i.kt)("h3",{id:"install-dependencies"},"Install Dependencies"),(0,i.kt)("p",null,"Note that these dependencies are only known to work on Ubuntu-based x64 machines."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"%%bash\nsudo apt-get update\nsudo apt-get install -y libgl1 libglib2.0-0 git-lfs\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\nIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\nHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\nHit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\nHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\nGet:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\nGet:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\nGet:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\nHit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\nGet:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,550 kB]\nGet:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\nGet:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\nGet:14 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,227 kB]\nHit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\nGet:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,164 kB]\nGet:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,012 kB]\nGet:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,328 kB]\nGet:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,444 kB]\nGet:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,187 kB]\nGet:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,109 kB]\nGet:22 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\nFetched 16.4 MB in 4s (3,885 kB/s)\nReading package lists...\nReading package lists...\nBuilding dependency tree...\nReading state information...\ngit-lfs is already the newest version (2.3.4-1).\nlibgl1 is already the newest version (1.0.0-2ubuntu2.3).\nlibglib2.0-0 is already the newest version (2.56.4-0ubuntu0.18.04.9).\nlibglib2.0-0 set to manually installed.\nThe following package was automatically installed and is no longer required:\n  libnvidia-common-460\nUse 'sudo apt autoremove' to remove it.\n0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n")),(0,i.kt)("h3",{id:"clone-the-repository-and-dependencies"},"Clone the Repository and Dependencies"),(0,i.kt)("p",null,"The following commands clone the example repository, other required repositories, and installs the Python dependencies."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"%%bash\ngit clone https://github.com/js-ts/stable_diffusion.openvino\ncd stable_diffusion.openvino\ngit lfs install\ngit clone https://huggingface.co/openai/clip-vit-large-patch14\ngit clone https://huggingface.co/bes-dev/stable-diffusion-v1-4-openvino\npip3 install -r requirements.txt\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Updated git hooks.\nGit LFS initialized.\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting numpy==1.19.5\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\nCollecting opencv-python==4.5.5.64\n  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\nCollecting transformers==4.16.2\n  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\nCollecting diffusers==0.2.4\n  Downloading diffusers-0.2.4-py3-none-any.whl (112 kB)\nCollecting tqdm==4.64.0\n  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\nCollecting openvino==2022.1.0\n  Downloading openvino-2022.1.0-7019-cp37-cp37m-manylinux_2_27_x86_64.whl (26.1 MB)\nCollecting huggingface_hub==0.9.0\n  Downloading huggingface_hub-0.9.0-py3-none-any.whl (120 kB)\nRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.7.3)\nCollecting streamlit==1.12.0\n  Downloading streamlit-1.12.0-py2.py3-none-any.whl (9.1 MB)\nCollecting watchdog==2.1.9\n  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (5.0.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (2.23.0)\nCollecting sacremoses\n  Downloading sacremoses-0.0.53.tar.gz (880 kB)\nCollecting tokenizers!=0.11.3,>=0.10.1\n  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (2022.6.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (3.8.0)\nRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from diffusers==0.2.4->-r requirements.txt (line 4)) (1.12.1+cu113)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from diffusers==0.2.4->-r requirements.txt (line 4)) (7.1.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub==0.9.0->-r requirements.txt (line 7)) (4.1.1)\nRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit==1.12.0->-r requirements.txt (line 9)) (0.10.2)\nCollecting rich>=10.11.0\n  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\nCollecting blinker>=1.0.0\n  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit==1.12.0->-r requirements.txt (line 9)) (2.8.2)\nRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.12.0->-r requirements.txt (line 9)) (4.2.4)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.12.0->-r requirements.txt (line 9)) (7.1.2)\nRequirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.12.0->-r requirements.txt (line 9)) (4.2.0)\nRequirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.12.0->-r requirements.txt (line 9)) (3.17.3)\nRequirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.12.0->-r requirements.txt (line 9)) (1.5.1)\nCollecting validators>=0.2\n  Downloading validators-0.20.0.tar.gz (30 kB)\nCollecting pydeck>=0.1.dev5\n  Downloading pydeck-0.8.0b4.dev1-py2.py3-none-any.whl (4.7 MB)\nRequirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.12.0->-r requirements.txt (line 9)) (6.0.1)\nRequirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.12.0->-r requirements.txt (line 9)) (1.3.5)\nCollecting semver\n  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\nCollecting pympler>=0.9\n  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\nCollecting gitpython!=3.1.19\n  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\nRequirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.12.0->-r requirements.txt (line 9)) (5.1.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.12.0->-r requirements.txt (line 9)) (2.11.3)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.12.0->-r requirements.txt (line 9)) (4.3.3)\nRequirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.12.0->-r requirements.txt (line 9)) (0.4)\nRequirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.12.0->-r requirements.txt (line 9)) (0.12.0)\nCollecting gitdb<5,>=4.0.1\n  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\nCollecting smmap<6,>=3.0.1\n  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2->-r requirements.txt (line 3)) (3.9.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.12.0->-r requirements.txt (line 9)) (0.18.1)\nRequirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.12.0->-r requirements.txt (line 9)) (5.10.0)\nRequirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.12.0->-r requirements.txt (line 9)) (22.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2->-r requirements.txt (line 3)) (3.0.9)\nRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit==1.12.0->-r requirements.txt (line 9)) (2022.4)\nRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12->streamlit==1.12.0->-r requirements.txt (line 9)) (1.15.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit==1.12.0->-r requirements.txt (line 9)) (2.0.1)\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (2022.9.24)\nRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.11.0->streamlit==1.12.0->-r requirements.txt (line 9)) (2.6.1)\nCollecting commonmark<0.10.0,>=0.9.0\n  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\nRequirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators>=0.2->streamlit==1.12.0->-r requirements.txt (line 9)) (4.4.2)\nRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2->-r requirements.txt (line 3)) (1.2.0)\nBuilding wheels for collected packages: validators, sacremoses\n  Building wheel for validators (setup.py): started\n  Building wheel for validators (setup.py): finished with status 'done'\n  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=26590051d891b62435bf2dacfe6f7892c2f6d31effc4d2bb29b64745dc2db715\n  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n  Building wheel for sacremoses (setup.py): started\n  Building wheel for sacremoses (setup.py): finished with status 'done'\n  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=d7d5bb4237a42691e3802c2eae97905bfafd668e1bec67a4fba4420dc7e4fef7\n  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\nSuccessfully built validators sacremoses\nInstalling collected packages: smmap, numpy, tqdm, gitdb, commonmark, watchdog, validators, tokenizers, semver, sacremoses, rich, pympler, pydeck, huggingface-hub, gitpython, blinker, transformers, streamlit, openvino, opencv-python, diffusers\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.64.1\n    Uninstalling tqdm-4.64.1:\n      Successfully uninstalled tqdm-4.64.1\n  Attempting uninstall: opencv-python\n    Found existing installation: opencv-python 4.6.0.66\n    Uninstalling opencv-python-4.6.0.66:\n      Successfully uninstalled opencv-python-4.6.0.66\nSuccessfully installed blinker-1.5 commonmark-0.9.1 diffusers-0.2.4 gitdb-4.0.9 gitpython-3.1.29 huggingface-hub-0.9.0 numpy-1.19.5 opencv-python-4.5.5.64 openvino-2022.1.0 pydeck-0.8.0b4.dev1 pympler-1.0.1 rich-12.6.0 sacremoses-0.0.53 semver-2.13.0 smmap-5.0.0 streamlit-1.12.0 tokenizers-0.13.1 tqdm-4.64.0 transformers-4.16.2 validators-0.20.0 watchdog-2.1.9\n\n\nfatal: destination path 'stable_diffusion.openvino' already exists and is not an empty directory.\nfatal: destination path 'clip-vit-large-patch14' already exists and is not an empty directory.\nCloning into 'stable-diffusion-v1-4-openvino'...\ntcmalloc: large alloc 1471086592 bytes == 0x557fcdf96000 @  0x7fbf33db52a4 0x557f9119511f 0x557f9117225b 0x557f91126f33 0x557f910cb22a 0x557f910cb6e6 0x557f910e8451 0x557f910e89e9 0x557f910e8f13 0x557f9118de12 0x557f9102f162 0x557f91015a65 0x557f91016725 0x557f9101572a 0x7fbf330fcc87 0x557f9101577a\ntcmalloc: large alloc 2206621696 bytes == 0x558025a86000 @  0x7fbf33db52a4 0x557f9119511f 0x557f9117225b 0x557f91126f33 0x557f910cb22a 0x557f910cb6e6 0x557f910e8451 0x557f910e89e9 0x557f910e8f13 0x557f9118de12 0x557f9102f162 0x557f91015a65 0x557f91016725 0x557f9101572a 0x7fbf330fcc87 0x557f9101577a\ntcmalloc: large alloc 3309936640 bytes == 0x5580a92ec000 @  0x7fbf33db52a4 0x557f9119511f 0x557f9117225b 0x557f91126f33 0x557f910cb22a 0x557f910cb6e6 0x557f910e8451 0x557f910e89e9 0x557f910e8f13 0x557f9118de12 0x557f9102f162 0x557f91015a65 0x557f91016725 0x557f9101572a 0x7fbf330fcc87 0x557f9101577a\ntcmalloc: large alloc 4964900864 bytes == 0x5581a09c6000 @  0x7fbf33db52a4 0x557f9119511f 0x557f9117225b 0x557f91126f33 0x557f910cb22a 0x557f910cb6e6 0x557f910e8451 0x557f910e89e9 0x557f910e8f13 0x557f9118de12 0x557f9102f162 0x557f91015a65 0x557f91016725 0x557f9101572a 0x7fbf330fcc87 0x557f9101577a\nFiltering content: 100% (4/4), 3.97 GiB | 31.06 MiB/s, done.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njaxlib 0.3.20+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njax 0.3.21 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\ncmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n")),(0,i.kt)("h3",{id:"generating-an-image"},"Generating an Image"),(0,i.kt)("p",null,"Now that we have all the dependencies installed, we can call the ",(0,i.kt)("inlineCode",{parentName:"p"},"demo.py")," wrapper, which is a simple CLI, to generate an image from a prompt."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'!cd stable_diffusion.openvino && \\\n  python3 demo.py --prompt "hello" --output hello.png\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\ntcmalloc: large alloc 3438092288 bytes == 0x31ba6000 @  0x7f97fabbd1e7 0x7f97e8f5d3f6 0x7f97e8c241e3 0x7f975be0f944 0x7f97e90ec49c 0x7f97e8d4e6e5 0x7f97e8cb9305 0x7f97e963a95e 0x7f97e966ad0e 0x58ec54 0x58fc01 0x51b7fd 0x5b41c5 0x4ba80a 0x537e46 0x58ff66 0x51bbc5 0x58f2a7 0x51740e 0x5b41c5 0x604133 0x606e06 0x606ecc 0x609aa6 0x64d332 0x64d4de 0x7f97fa7bac87 0x5b561a\n32it [11:09, 20.92s/it]\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import IPython.display as display\ndisplay.Image("stable_diffusion.openvino/hello.png")\n')),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"png",src:n(216).Z,width:"512",height:"512"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'!cd stable_diffusion.openvino && \\\n  python3 demo.py --prompt "cat driving a car" --output cat.png\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\ntcmalloc: large alloc 3438092288 bytes == 0x3202e000 @  0x7fac3f2091e7 0x7fac2d5a93f6 0x7fac2d2701e3 0x7faba045b944 0x7fac2d73849c 0x7fac2d39a6e5 0x7fac2d305305 0x7fac2dc8695e 0x7fac2dcb6d0e 0x58ec54 0x58fc01 0x51b7fd 0x5b41c5 0x4ba80a 0x537e46 0x58ff66 0x51bbc5 0x58f2a7 0x51740e 0x5b41c5 0x604133 0x606e06 0x606ecc 0x609aa6 0x64d332 0x64d4de 0x7fac3ee06c87 0x5b561a\n32it [11:13, 21.06s/it]\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import IPython.display as display\ndisplay.Image("stable_diffusion.openvino/cat.png")\n')),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"png",src:n(8374).Z,width:"512",height:"512"})),(0,i.kt)("h2",{id:"2-running-stable-diffusion-cpu-on-bacalhau"},"2. Running Stable Diffusion (CPU) on Bacalhau"),(0,i.kt)("p",null,"Now we have a working example, we can convert it into a format that allows us to perform inference in a distributed environment."),(0,i.kt)("p",null,"First we will create a ",(0,i.kt)("inlineCode",{parentName:"p"},"Dockerfile")," to containerize the inference code. The Dockerfile ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/js-ts/stable_diffusion.openvino/blob/master/Dockerfile"},"can be found in the repository"),", but is presented here to aid understanding."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-Dockerfile"},'FROM python:3.9.9-bullseye\n\nWORKDIR /src\n\nRUN apt-get update && \\\n    apt-get install -y \\\n    libgl1 libglib2.0-0 git-lfs\n\nRUN git lfs install\n\nCOPY requirements.txt /src/\n\nRUN pip3 install -r requirements.txt\n\nCOPY stable_diffusion_engine.py demo.py demo_web.py /src/\nCOPY data/ /src/data/\n\nRUN git clone https://huggingface.co/openai/clip-vit-large-patch14\nRUN git clone https://huggingface.co/bes-dev/stable-diffusion-v1-4-openvino\n\n# download models\nRUN python3 demo.py --num-inference-steps 1 --prompt "test" --output /tmp/test.jpg\n')),(0,i.kt)("p",null,"This container is using the ",(0,i.kt)("inlineCode",{parentName:"p"},"python:3.9.9-bullseye")," image and the working directory is set. Next the Dockerfile installs the same dependencies from earlier in this notebook. Then we add our custom code and pull the dependent repositories."),(0,i.kt)("p",null,"We've already pushed this image to GHCR, but for posterity, you'd use a command like this to update it:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"docker buildx build --platform linux/amd64 --push -t ghcr.io/bacalhau-project/examples/stable-diffusion-cpu:0.0.1 .\n")),(0,i.kt)("h3",{id:"prerequisites-1"},"Prerequisites"),(0,i.kt)("p",null,"To run this example you will need:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://www.bacalhau.org/"},"Bacalhau")," installed and running")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"!command -v bacalhau >/dev/null 2>&1 || (export BACALHAU_INSTALL_DIR=.; curl -sL https://get.bacalhau.org/install.sh | bash)\npath=!echo $PATH\n%env PATH=./:{path[0]}\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"env: PATH=./:/Users/phil/.pyenv/versions/3.9.7/bin:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.gvm/bin:/opt/homebrew/opt/findutils/libexec/gnubin:/opt/homebrew/opt/coreutils/libexec/gnubin:/opt/homebrew/Caskroom/google-cloud-sdk/latest/google-cloud-sdk/bin:/Users/phil/.pyenv/shims:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/MacGPG2/bin:/Users/phil/.nexustools\n")),(0,i.kt)("h3",{id:"generating-an-image-using-stable-diffusion-on-bacalhau"},"Generating an Image Using Stable Diffusion on Bacalhau"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://www.bacalhau.org/"},"Bacalhau")," is a distributed computing platform that allows you to run jobs on a network of computers. It is designed to be easy to use and to run on a variety of hardware. In this example, we will use it to run the stable diffusion model on a CPU."),(0,i.kt)("p",null,"To submit a job, you can use the Bacalhau CLI. The following command passes a prompt to the model and generates an image in the outputs directory."),(0,i.kt)("admonition",{type:"tip"},(0,i.kt)("p",{parentName:"admonition"},"This will take about 10 minutes to complete. Go grab a coffee. Or a beer. Or both. If you want to block and wait for the job to complete, add the ",(0,i.kt)("inlineCode",{parentName:"p"},"--wait")," flag."),(0,i.kt)("p",{parentName:"admonition"},"Furthermore, the container itself is about 15GB, so it might take a while to download on the node if it isn't cached.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'%%bash --out job_id\nbacalhau docker run ghcr.io/bacalhau-project/examples/stable-diffusion-cpu:0.0.1 --id-only -- python demo.py --prompt "First Humans On Mars" --output ../outputs/mars.png\n')),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"%env JOB_ID={job_id}\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"env: JOB_ID=ff7b104d-0736-434e-8e5f-dc47c457cbf5\n")),(0,i.kt)("p",null,"Running the commands will output a UUID that represents the job that was created. You can check the status of the job with the following command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"%%bash\nbacalhau list --id-filter ${JOB_ID}\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"\x1b[92;100m CREATED  \x1b[0m\x1b[92;100m ID       \x1b[0m\x1b[92;100m JOB                     \x1b[0m\x1b[92;100m STATE     \x1b[0m\x1b[92;100m VERIFIED \x1b[0m\x1b[92;100m PUBLISHED               \x1b[0m\n\x1b[97;40m 14:14:05 \x1b[0m\x1b[97;40m ff7b104d \x1b[0m\x1b[97;40m Docker jsacex/stable... \x1b[0m\x1b[97;40m Completed \x1b[0m\x1b[97;40m          \x1b[0m\x1b[97;40m /ipfs/QmWxg6CXqZhjAa... \x1b[0m\n")),(0,i.kt)("p",null,"Wait until it says ",(0,i.kt)("inlineCode",{parentName:"p"},"Completed")," and then get the results."),(0,i.kt)("p",null,"To find out more information about your job, run the following command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"%%bash\nbacalhau describe ${JOB_ID}\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'APIVersion: V1alpha1\nClientID: 77cf46c04f88ffb1c3e0e4b6e443724e8d2d87074d088ef1a6294a448fa85d2e\nCreatedAt: "2022-10-13T14:14:05.447388688Z"\nDeal:\n  Concurrency: 1\nExecutionPlan:\n  ShardsTotal: 1\nID: ff7b104d-0736-434e-8e5f-dc47c457cbf5\nJobState:\n  Nodes:\n    QmYgxZiySj3MRkwLSL4X2MF5F9f2PMhAE3LV49XkfNL1o3:\n      Shards:\n        "0":\n          NodeId: QmYgxZiySj3MRkwLSL4X2MF5F9f2PMhAE3LV49XkfNL1o3\n          PublishedResults:\n            CID: QmWxg6CXqZhjAagQVxqopwhyXMHAHvTFKXYtWjzc7BJp65\n            Name: job-ff7b104d-0736-434e-8e5f-dc47c457cbf5-shard-0-host-QmYgxZiySj3MRkwLSL4X2MF5F9f2PMhAE3LV49XkfNL1o3\n            StorageSource: IPFS\n          RunOutput:\n            exitCode: 0\n            runnerError: ""\n            stderr: "ftfy or spacy is not installed using BERT BasicTokenizer instead\n              of ftfy.\\n\\r0it [00:00, ?it/s]\\r1it [00:05,  5.26s/it]\\r2it [00:09,\n              \\ 4.95s/it]\\r3it [00:14,  4.96s/it]\\r4it [00:19,  4.84s/it]\\r5it [00:24,\n              \\ 4.92s/it]\\r6it [00:29,  4.83s/it]\\r7it [00:34,  4.84s/it]\\r8it [00:39,\n              \\ 4.84s/it]\\r9it [00:43,  4.78s/it]\\r10it [00:48,  4.78s/it]\\r11it [00:53,\n              \\ 4.77s/it]\\r12it [00:58,  4.78s/it]\\r13it [01:02,  4.76s/it]\\r14it\n              [01:07,  4.77s/it]\\r15it [01:12,  4.81s/it]\\r16it [01:17,  4.94s/it]\\r17it\n              [01:22,  4.92s/it]\\r18it [01:27,  4.93s/it]\\r19it [01:32,  4.91s/it]\\r20it\n              [01:37,  4.90s/it]\\r21it [01:42,  4.87s/it]\\r22it [01:46,  4.81s/it]\\r23it\n              [01:51,  4.81s/it]\\r24it [01:56,  4.76s/it]\\r25it [02:01,  4.80s/it]\\r26it\n              [02:05,  4.71s/it]\\r27it [02:10,  4.65s/it]\\r28it [02:14,  4.56s/it]\\r29it\n              [02:18,  4.52s/it]\\r30it [02:23,  4.60s/it]\\r31it [02:28,  4.60s/it]\\r32it\n              [02:32,  4.62s/it]\\r32it [02:32,  4.78s/it]"\n            stderrtruncated: false\n            stdout: ""\n            stdouttruncated: false\n          State: Completed\n          Status: \'Got results proposal of length: 0\'\n          VerificationResult:\n            Complete: true\n            Result: true\n    QmdZQ7ZbhnvWY1J12XYKGHApJ6aufKyLNSvf8jZBrBaAVL:\n      Shards:\n        "0":\n          NodeId: QmdZQ7ZbhnvWY1J12XYKGHApJ6aufKyLNSvf8jZBrBaAVL\n          PublishedResults: {}\n          State: Cancelled\n          VerificationResult: {}\nRequesterNodeID: QmXaXu9N5GNetatsvwnTfQqNtSeKAD6uCmarbh3LMRYAcF\nRequesterPublicKey: CAASpgIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCehDIWl72XKJi1tsrYM9JjAWt3n6hNzrCA+IVRXixK1sJVTLMpsxEP8UKJI+koAWkAUuY8yi6DMzot0owK4VpM3PYp34HdKi2hTjzM8pjCVb70XVXt6k9bzj4KmbiQTuEkQfvwIRmgxb2jrkRdTpZmhMb1Q7StR/nrGa/bx75Vpupx1EYH6+LixYnnV5WbCUK/kjpBW8SF5v+f9ZO61KHd9DMpdhJnzocTGq17tAjHh3birke0xlP98JjxlMkzzvIAuFsnH0zBIgjmHDA1Yi5DcOPWgE0jUfGlSDC1t2xITVoofHQcXDjkHZE6OhxswNYPd7cnTf9OppLddFdQnga5AgMBAAE=\nSpec:\n  Docker:\n    Entrypoint:\n    - python\n    - demo.py\n    - --prompt\n    - First Humans On Mars\n    - --output\n    - ../outputs/mars.png\n    Image: ghcr.io/bacalhau-project/examples/stable-diffusion-cpu:0.0.1\n  Engine: Docker\n  Language:\n    JobContext: {}\n  Publisher: Estuary\n  Resources:\n    GPU: ""\n  Sharding:\n    BatchSize: 1\n    GlobPatternBasePath: /inputs\n  Verifier: Noop\n  outputs:\n  - Name: outputs\n    StorageSource: IPFS\n    path: /outputs\n')),(0,i.kt)("p",null,"If you see that the job has completed and there are no errors, then you can download the results with the following command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"%%bash\nrm -rf results && mkdir -p results\nbacalhau get $JOB_ID --output-dir results\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Fetching results of job 'ff7b104d-0736-434e-8e5f-dc47c457cbf5'...\n")),(0,i.kt)("p",null,"After the download has finished you should\nsee the following contents in results directory:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"%%bash\nls results/volumes/outputs\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"mars.png\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import IPython.display as display\ndisplay.Image("results/volumes/outputs/mars.png")\n')),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"png",src:n(3835).Z,width:"512",height:"512"})))}d.isMDXComponent=!0},1121:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/cod-20eab047ac8cdbd4a1345bcdb269b4e0.png"},8374:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/index_10_0-fea9a9996ca891e993b0f7f5f5cf4d24.png"},3835:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/index_24_0-c2e2786333fb3f08d08a6a435084100d.png"},216:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/index_8_0-2b8181632cc5143706e0bd8203496e06.png"}}]);