"use strict";(self.webpackChunkbacalhau_docs=self.webpackChunkbacalhau_docs||[]).push([[5460],{3905:(e,t,a)=>{a.d(t,{Zo:()=>i,kt:()=>h});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var c=n.createContext({}),u=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},i=function(e){var t=u(e.components);return n.createElement(c.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},g=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,c=e.parentName,i=l(e,["components","mdxType","originalType","parentName"]),p=u(a),g=o,h=p["".concat(c,".").concat(g)]||p[g]||d[g]||r;return a?n.createElement(h,s(s({ref:t},i),{},{components:a})):n.createElement(h,s({ref:t},i))}));function h(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,s=new Array(r);s[0]=g;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l[p]="string"==typeof e?e:o,s[1]=l;for(var u=2;u<r;u++)s[u]=a[u];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}g.displayName="MDXCreateElement"},6598:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>l,toc:()=>u});var n=a(7462),o=(a(7294),a(3905));const r={},s="The Bacalhau Job",l={unversionedId:"case-studies/duckdb-logs-processing/The Bacalhau Job/The-Bacalhau-Job",id:"case-studies/duckdb-logs-processing/The Bacalhau Job/The-Bacalhau-Job",title:"The Bacalhau Job",description:"Setting Up Bacalhau On the Provisioned Nodes",source:"@site/docs/case-studies/duckdb-logs-processing/The Bacalhau Job/The-Bacalhau-Job.md",sourceDirName:"case-studies/duckdb-logs-processing/The Bacalhau Job",slug:"/case-studies/duckdb-logs-processing/The Bacalhau Job/The-Bacalhau-Job",permalink:"/case-studies/duckdb-logs-processing/The Bacalhau Job/The-Bacalhau-Job",draft:!1,editUrl:"https://github.com/bacalhau-project/docs.bacalhau.org/blob/main/docs/case-studies/duckdb-logs-processing/The Bacalhau Job/The-Bacalhau-Job.md",tags:[],version:"current",lastUpdatedAt:1685025044,formattedLastUpdatedAt:"May 25, 2023",frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Setup Fake Log Generation",permalink:"/case-studies/duckdb-logs-processing/Fake-Log-Generation/set-up-a-fake-log-creating-job"},next:{title:"Home",permalink:"/"}},c={},u=[{value:"Setting Up Bacalhau On the Provisioned Nodes",id:"setting-up-bacalhau-on-the-provisioned-nodes",level:2},{value:"Setting Up the Compute Nodes",id:"setting-up-the-compute-nodes",level:2},{value:"Running the Log Processing Job on the nodes using bacalhau",id:"running-the-log-processing-job-on-the-nodes-using-bacalhau",level:2},{value:"Create the <strong>Log Processing</strong> Script",id:"create-the-log-processing-script",level:3}],i={toc:u},p="wrapper";function d(e){let{components:t,...r}=e;return(0,o.kt)(p,(0,n.Z)({},i,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"the-bacalhau-job"},"The Bacalhau Job"),(0,o.kt)("h2",{id:"setting-up-bacalhau-on-the-provisioned-nodes"},"Setting Up Bacalhau On the Provisioned Nodes"),(0,o.kt)("p",null,"Run this command on the requestor node in our case it\u2019s the"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau serve  --job-selection-accept-networked\n")),(0,o.kt)("p",null,"Copy the command from the requestor node and add the path to the logs directory and the networked flag to the end then paste it into compute node terminals:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Serve Image",src:a(685).Z,width:"1500",height:"402"})),(0,o.kt)("p",null,"alternatively you could also ssh into into the nodes using gcloud and run the command to start bacalhau requestor node node"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'gcloud compute ssh \\\n  --zone "us-central1-a" \\\n  "us-central1-a-8gb-node" \\\n  --project "bacalhau-development" \\\n  --command="bacalhau serve  --job-selection-accept-networked"\n')),(0,o.kt)("p",null,"copy the highlighted command and add these flags to the end of the copied command:\n",(0,o.kt)("inlineCode",{parentName:"p"},'--allow-listed-local-paths "/logrotate/logs/**" --job-selection-accept-networked')),(0,o.kt)("h2",{id:"setting-up-the-compute-nodes"},"Setting Up the Compute Nodes"),(0,o.kt)("p",null,"you can either paste this command into the terminals of your compute nodes"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"sudo bacalhau serve \\\n    --node-type compute \\\n    --private-internal-ipfs \\\n    --peer /ip4/10.128.15.197/tcp/1235/p2p/Qme3YZsBnimme5vY8Ye5w3NV3aSJ6m3h2KPzNSC4Y7orGT \\\n    --ipfs-swarm-addr /ip4/10.128.15.197/tcp/43475/p2p/QmU4wdEDbr8bJ1799r5YXX2NCA5vaX2UbeD3AJAurREuoE \\\n    --allow-listed-local-paths '/logrotate/logs/**' \\\n    --job-selection-accept-networked\n")),(0,o.kt)("p",null,"OR"),(0,o.kt)("p",null,"SSH into the nodes using gcloud CLI and run the command"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'gcloud compute ssh \\\n  --zone "us-central1-a" \\\n  "us-central1-a-node" \\\n  --project "bacalhau-development" \\\n  --command="sudo bacalhau serve \\\n    --node-type compute \\\n    --private-internal-ipfs \\\n    --peer /ip4/10.128.15.197/tcp/1235/p2p/Qme3YZsBnimme5vY8Ye5w3NV3aSJ6m3h2KPzNSC4Y7orGT \\\n    --ipfs-swarm-addr /ip4/10.128.15.197/tcp/43475/p2p/QmU4wdEDbr8bJ1799r5YXX2NCA5vaX2UbeD3AJAurREuoE \\\n    --allow-listed-local-paths \'/logrotate/logs/**\' \\\n    --job-selection-accept-networked"\n')),(0,o.kt)("h2",{id:"running-the-log-processing-job-on-the-nodes-using-bacalhau"},"Running the Log Processing Job on the nodes using bacalhau"),(0,o.kt)("h3",{id:"create-the-log-processing-script"},"Create the ",(0,o.kt)("strong",{parentName:"h3"},"Log Processing")," Script"),(0,o.kt)("p",null,"The log processing script which uses duckdb and saves the outputs to Archival Storage"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import json\nimport os\nfrom datetime import datetime\nimport duckdb\nimport tempfile\nimport argparse\nimport requests\nimport pandas as pd\nfrom google.cloud import storage\n\ndef get_metadata(metadata_name):\n    metadata_server_token_url = \"http://metadata/computeMetadata/v1/instance/service-accounts/default/token\"\n    token_request_headers = {'Metadata-Flavor': 'Google'}\n    token_response = requests.get(metadata_server_token_url, headers=token_request_headers)\n    jwt = token_response.json()['access_token']\n\n    metadata_server_url = f\"http://metadata.google.internal/computeMetadata/v1/instance/{metadata_name}\"\n    metadata_request_headers = {'Metadata-Flavor': 'Google', 'Authorization': f'Bearer {jwt}'}\n\n    return requests.get(metadata_server_url, headers=metadata_request_headers).text\n\ndef main(input_file, query):\n    # Create an in-memory DuckDB database\n    con = duckdb.connect(database=':memory:', read_only=False)\n\n    # Create a table from the JSON data\n    con.execute(f\"CREATE TABLE log_data AS SELECT * FROM read_json('{input_file}', \"\n                f\"auto_detect=false, \"\n                f\"columns={json.dumps({'id': 'varchar', '@timestamp': 'varchar', '@version': 'varchar', 'message': 'varchar'})})\")\n\n    # Execute the DuckDB query on the log data\n    result = con.execute(query).fetchdf()\n\n    # Convert the result to JSON\n    result_json = result.to_json(orient='records')\n\n    # Generate the output file name\n    output_file_name = f\"{get_metadata('name')}-Security-{datetime.now().strftime('%Y%m%d%H%M')}.json\"\n\n    # Get the region from the metadata server\n    region = get_metadata(\"zone\").split(\"/\")[3]\n\n    # Generate the bucket name\n    bucket_name = f\"{region}-node-archive-bucket\"\n\n    # Write the result to a temporary file\n    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as temp:\n        temp.write(result_json)\n        temp.close()\n\n        # Upload the file to GCP bucket\n        storage_client = storage.Client()\n        bucket = storage_client.get_bucket(bucket_name)\n        blob = bucket.blob(output_file_name)\n        blob.upload_from_filename(temp.name)\n\n        # Remove the temporary file\n        os.remove(temp.name)\n\nif __name__ == \"__main__\":\n    # Set up the argument parser\n    parser = argparse.ArgumentParser(description=\"Process log data\")\n    parser.add_argument(\"input_file\", help=\"Path to the input log file\")\n    parser.add_argument(\"--query\", default=\"SELECT * FROM log_data WHERE message LIKE '%[SECURITY]%' ORDER BY \\\"@timestamp\\\"\", \n                        help=\"SQL query to execute on the log data\")\n\n    # Parse the command-line arguments\n    args = parser.parse_args()\n\n    # Call the main function\n    main(args.input_file, args.query)\n")),(0,o.kt)("p",null,"Testing the script locally"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"# downloading a fake log file\nwget https://gist.githubusercontent.com/js-ts/93ab7bf161c4908952a17758e38c11d6/raw/6ccafd337b5f3c3b1aa4e54a55ea36be8c6ac30c/fake_logs.log\n#testing the script\npython3 process.py fake_logs.log\n")),(0,o.kt)("p",null,"Building the container to run the script on bacalhau"),(0,o.kt)("p",null,"Dockerfile"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'FROM davidgasquez/duckdb:latest\n\nRUN apt-get update && apt-get install -y python3 python3-pip curl wget lsb-release gnupg\n\n# Install Google Cloud SDK and gsutil\nRUN echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \\\n    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && \\\n    apt-get update -y && apt-get install google-cloud-sdk -y\n\nRUN pip3 install duckdb requests google-cloud-storage pandas\n\nRUN apt-get update -y && apt-get install curl wget -y\n\nWORKDIR /\n# You can copy the script instead of downlaoding it\n# COPY ./process.py /process.py\nRUN wget https://gist.githubusercontent.com/js-ts/cd0f5816aacbc50b2dc692e1c7f4bc90/raw/07e0ceffe72031b5dfb76a53df4a81e533b495b3/process.py\n')),(0,o.kt)("p",null,"Building the container"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"docker build -t <your-docker-user>/<image-name> .\n")),(0,o.kt)("p",null,"Replace your-dockerhub-username with your actual DockerHub username. This command will build the Docker image and tag it with your DockerHub username and the image name"),(0,o.kt)("p",null,"Push the Docker Image to DockerHub Once the build process is complete, Next, push the Docker image to DockerHub using the following command:"),(0,o.kt)("p",null,"Again, replace your-dockerhub-username with your actual DockerHub username. This command will push the Docker image to your DockerHub repository."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"docker push <your-docker-user>/<image-name>\n")),(0,o.kt)("p",null,"After that copy these envs from the requestor node and paste it into your terminal"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"export BACALHAU_IPFS_SWARM_ADDRESSES=/ip4/10.128.15.197/tcp/44111/p2p/QmVKciUngasyoPmgJrpGkdrUjSu6n9Tx3gDv3Th62UeUwi\nexport BACALHAU_API_HOST=35.224.115.23 #Replace this with the Static IP of the requestor node\nexport BACALHAU_API_PORT=1234\n")),(0,o.kt)("p",null,"You can then run the job"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"bacalhau docker run  \\\n--concurrency 3 \\\n --network=full \\\n -w /inputs \\\n -i file:///logrotate/logs \\\njsacex/duckdb-logs-archive-query \\\n-- /bin/bash -c \"python3 /process.py --query \\\"SELECT * FROM log_data WHERE message LIKE '%[SECURITY]%' ORDER BY '@timestamp'\\\" /inputs/fake_logs.log\"\n")),(0,o.kt)("p",null,"Viewing the Processed Log Output of a Single Node"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"gsutil cat gs://us-east1-b-node-archive-bucket/us-east1-b-node-Security-202305220729.json | head -n 10\n")))}d.isMDXComponent=!0},685:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/serve-image-fd5f25a19851f35d6369006aabf70afa.png"}}]);