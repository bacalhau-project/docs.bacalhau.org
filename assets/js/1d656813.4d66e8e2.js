"use strict";(self.webpackChunkbacalhau_docs=self.webpackChunkbacalhau_docs||[]).push([[6957],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>c});var a=n(7294);function s(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){s(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function r(e,t){if(null==e)return{};var n,a,s=function(e,t){if(null==e)return{};var n,a,s={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(s[n]=e[n]);return s}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(s[n]=e[n])}return s}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,s=e.mdxType,o=e.originalType,l=e.parentName,u=r(e,["components","mdxType","originalType","parentName"]),m=p(n),c=s,h=m["".concat(l,".").concat(c)]||m[c]||d[c]||o;return n?a.createElement(h,i(i({ref:t},u),{},{components:n})):a.createElement(h,i({ref:t},u))}));function c(e,t){var n=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var o=n.length,i=new Array(o);i[0]=m;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r.mdxType="string"==typeof e?e:s,i[1]=r;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},5328:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>p});var a=n(7462),s=(n(7294),n(3905));const o={},i="Running BIDS Apps on bacalhau",r={unversionedId:"examples/miscellaneous/BIDS/index",id:"examples/miscellaneous/BIDS/index",title:"Running BIDS Apps on bacalhau",description:"What is a BIDS App? (source)",source:"@site/docs/examples/miscellaneous/BIDS/index.md",sourceDirName:"examples/miscellaneous/BIDS",slug:"/examples/miscellaneous/BIDS/",permalink:"/examples/miscellaneous/BIDS/",draft:!1,editUrl:"https://github.com/bacalhau-project/docs.bacalhau.org/blob/main/docs/examples/miscellaneous/BIDS/index.md",tags:[],version:"current",lastUpdatedAt:1676470484,formattedLastUpdatedAt:"Feb 15, 2023",frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Miscellaneous",permalink:"/examples/miscellaneous/"},next:{title:"Coresets On Bacalhau",permalink:"/examples/miscellaneous/Coreset/"}},l={},p=[{value:"<strong>Downloading datasets</strong>",id:"downloading-datasets",level:2},{value:"<strong>Uploading the datasets to IPFS</strong>",id:"uploading-the-datasets-to-ipfs",level:3}],u={toc:p};function d(e){let{components:t,...n}=e;return(0,s.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"running-bids-apps-on-bacalhau"},"Running BIDS Apps on bacalhau"),(0,s.kt)("h1",{id:"introduction"},"Introduction"),(0,s.kt)("p",null,"What is a BIDS App? (",(0,s.kt)("a",{parentName:"p",href:"https://bids-apps.neuroimaging.io/about/"},"source"),")"),(0,s.kt)("p",null,"A BIDS App is a container image capturing a neuroimaging pipeline that takes a BIDS formatted dataset as input. BIDS (Brain Imaging Data Structure) is an emerging standard for organizing and describing neuroimaging datasets. Each BIDS App has the same core set of command line arguments, making them easy to run and integrate into automated platforms. BIDS Apps are constructed in a way that does not depend on any software outside of the image other than the container engine."),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/bacalhau-project/examples/blob/main/miscellaneous/BIDS/index.ipynb"},(0,s.kt)("img",{parentName:"a",src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})),"\n",(0,s.kt)("a",{parentName:"p",href:"https://mybinder.org/v2/gh/bacalhau-project/examples/HEAD?labpath=miscellaneous/BIDS/index.ipynb"},(0,s.kt)("img",{parentName:"a",src:"https://mybinder.org/badge.svg",alt:"Open In Binder"}))),(0,s.kt)("h2",{id:"downloading-datasets"},(0,s.kt)("strong",{parentName:"h2"},"Downloading datasets")),(0,s.kt)("p",null,"You can find the bids datasets in this google drive folder ",(0,s.kt)("a",{parentName:"p",href:"https://drive.google.com/drive/folders/0B2JWN60ZLkgkMGlUY3B4MXZIZW8?resourcekey=0-EYVSOlRbxeFKO8NpjWWM3w"},"archives")," "),(0,s.kt)("p",null,"download the relevant data, ",(0,s.kt)("a",{parentName:"p",href:"https://drive.google.com/drive/folders/0B2JWN60ZLkgkMGlUY3B4MXZIZW8"},"ds005.tar"),", and untar it in a directory. ",(0,s.kt)("inlineCode",{parentName:"p"},"ds005")," will be our input directory in the following example."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"data\n\u2514\u2500\u2500 ds005\n")),(0,s.kt)("h3",{id:"uploading-the-datasets-to-ipfs"},(0,s.kt)("strong",{parentName:"h3"},"Uploading the datasets to IPFS")),(0,s.kt)("p",null,"Upload the directory to IPFS using IPFS CLI (",(0,s.kt)("a",{parentName:"p",href:"https://docs.ipfs.tech/install/command-line/#official-distributions"},"Installation Instructions"),")"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"$ ipfs add -r data\nadded QmdsFcNbja8vbeNEj6HGfbvJmuu3cXUmgV4CR3HQqNqsNK data/ds005/CHANGES\n                                    .\n                                    .\n                                    .\nadded QmdnMxSSvD8QYR6F4S7wkgQsW16bR6U7zyDTbiEm72RPpB data/ds005\nadded QmaNyzSpJCt1gMCQLd3QugihY6HzdYmA8QMEa45LDBbVPz data\n 1.77 GiB / 1.77 GiB [=========================================================================================] 100.00%\n")),(0,s.kt)("p",null,"Copy the CID in the end which is ",(0,s.kt)("inlineCode",{parentName:"p"},"QmaNyzSpJCt1gMCQLd3QugihY6HzdYmA8QMEa45LDBbVPz")),(0,s.kt)("p",null,"Upload the directory to IPFS using ",(0,s.kt)("a",{parentName:"p",href:"https://app.pinata.cloud/"},"Pinata")," (Recommended)"),(0,s.kt)("p",null,"Click on the upload folder button and select the bids datasets folder that you want to upload"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://i.imgur.com/btnHw3N.png",alt:null})),(0,s.kt)("p",null,"After the Upload has finished copy the CID (highlighted part)"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://i.imgur.com/rETHXXz.png",alt:null})),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"!mkdir data\n!wget https://dist.ipfs.io/go-ipfs/v0.4.2/go-ipfs_v0.4.2_linux-amd64.tar.gz\n!tar xvfz go-ipfs_v0.4.2_linux-amd64.tar.gz\n!mv go-ipfs/ipfs /usr/local/bin/ipfs\n!cd data\n!ipfs init\n!ipfs cat /ipfs/QmYwAPJzv5CZsnA625s3Xf2nemtYgPpHdWEz79ojWnPbdG/readme\n!ipfs config Addresses.Gateway /ip4/127.0.0.1/tcp/8082\n!nohup ipfs daemon > startup.log &\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"--2022-09-30 19:10:07--  https://dist.ipfs.io/go-ipfs/v0.4.2/go-ipfs_v0.4.2_linux-amd64.tar.gz\nResolving dist.ipfs.io (dist.ipfs.io)... 209.94.78.1, 2602:fea2:3::1\nConnecting to dist.ipfs.io (dist.ipfs.io)|209.94.78.1|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7642422 (7.3M) [application/gzip]\nSaving to: \u2018go-ipfs_v0.4.2_linux-amd64.tar.gz.1\u2019\n\ngo-ipfs_v0.4.2_linu 100%[===================>]   7.29M  40.8MB/s    in 0.2s    \n\n2022-09-30 19:10:07 (40.8 MB/s) - \u2018go-ipfs_v0.4.2_linux-amd64.tar.gz.1\u2019 saved [7642422/7642422]\n\ngo-ipfs/build-log\ngo-ipfs/install.sh\ngo-ipfs/ipfs\ngo-ipfs/LICENSE\ngo-ipfs/README.md\ninitializing ipfs node at /root/.ipfs\nError: ipfs configuration file already exists!\nReinitializing would overwrite your keys.\n\nHello and Welcome to IPFS!\n\n\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\n\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\n\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n\u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d     \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nIf you're seeing this, you have successfully installed\nIPFS and are now interfacing with the ipfs merkledag!\n\n -------------------------------------------------------\n| Warning:                                              |\n|   This is alpha software. Use at your own discretion! |\n|   Much is missing or lacking polish. There are bugs.  |\n|   Not yet secure. Read the security notes for more.   |\n -------------------------------------------------------\n\nCheck out some of the other files in this directory:\n\n  ./about\n  ./help\n  ./quick-start     <-- usage examples\n  ./readme          <-- this file\n  ./security-notes\nnohup: redirecting stderr to stdout\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"!cd data\n!ipfs get QmdnMxSSvD8QYR6F4S7wkgQsW16bR6U7zyDTbiEm72RPpB\n")),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Running the command on bacalhau")),(0,s.kt)("p",null,"The command can be broken down into 4 pieces"),(0,s.kt)("p",null,(0,s.kt)("inlineCode",{parentName:"p"},"bacalhau docker run")," using the docker backend"),(0,s.kt)("p",null,(0,s.kt)("inlineCode",{parentName:"p"},"-v QmaNyzSpJCt1gMCQLd3QugihY6HzdYmA8QMEa45LDBbVPz:/data")," here we mount the CID of the dataset we uploaded to IPFS and mount it to a folder called data on the container"),(0,s.kt)("p",null,(0,s.kt)("inlineCode",{parentName:"p"},"nipreps/mriqc:latest")," the name and the tag of the docker image we are using"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"mriqc ../data/ds005 ../outputs participant --participant_label 01 02 03\n")),(0,s.kt)("p",null,"This is the command that we run where we specify path to the ",(0,s.kt)("inlineCode",{parentName:"p"},"../data/ds005")," input dataset"),(0,s.kt)("p",null,(0,s.kt)("inlineCode",{parentName:"p"},"../outputs")," path where we want to save our outputs,"),(0,s.kt)("p",null,(0,s.kt)("inlineCode",{parentName:"p"},"participant --participant_label 01 02 03")," Run the participant level in subjects 001 002 003"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"bacalhau docker run \\\n-v QmaNyzSpJCt1gMCQLd3QugihY6HzdYmA8QMEa45LDBbVPz:/data \\\nnipreps/mriqc:latest \\\n-- mriqc ../data/ds005 ../outputs participant --participant_label 01 02 03\n")),(0,s.kt)("p",null,"Insalling bacalhau"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"!curl -sL https://get.bacalhau.org/install.sh | bash\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Your system is linux_amd64\nNo BACALHAU detected. Installing fresh BACALHAU CLI...\nGetting the latest BACALHAU CLI...\nInstalling v0.2.3 BACALHAU CLI...\nDownloading https://github.com/filecoin-project/bacalhau/releases/download/v0.2.3/bacalhau_v0.2.3_linux_amd64.tar.gz ...\nDownloading sig file https://github.com/filecoin-project/bacalhau/releases/download/v0.2.3/bacalhau_v0.2.3_linux_amd64.tar.gz.signature.sha256 ...\nVerified OK\nExtracting tarball ...\nNOT verifying Bin\nbacalhau installed into /usr/local/bin successfully.\nClient Version: v0.2.3\nServer Version: v0.2.3\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"%%bash --out job_id\nbacalhau docker run \\\n--id-only \\ \n--wait \\\n--timeout 3600 \\\n--wait-timeout-secs 3600 \\\n-v QmaNyzSpJCt1gMCQLd3QugihY6HzdYmA8QMEa45LDBbVPz:/data \\\nnipreps/mriqc:latest \n-- mriqc ../data/ds005 ../outputs participant --participant_label 01 02 03\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"%env JOB_ID={job_id}\n")),(0,s.kt)("p",null,"Running the commands will output a UUID (like ",(0,s.kt)("inlineCode",{parentName:"p"},"54506541-4eb9-45f4-a0b1-ea0aecd34b3e"),"). This is the ID of the job that was created. You can check the status of the job with the following command:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"%%bash\nbacalhau list --id-filter ${JOB_ID} --wide\n")),(0,s.kt)("p",null,'Where it says "',(0,s.kt)("inlineCode",{parentName:"p"},"Completed"),'", that means the job is done, and we can get the results.'),(0,s.kt)("p",null,"To find out more information about your job, run the following command:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"%%bash\nbacalhau describe ${JOB_ID}\n")),(0,s.kt)("p",null,"To Download the results of your job, run "),(0,s.kt)("hr",null),(0,s.kt)("p",null,"the following command:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"%%bash\nrm -rf results && mkdir -p results\nbacalhau get $JOB_ID --output-dir results\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\x1b[90m12:19:36.609 |\x1b[0m \x1b[32mINF\x1b[0m \x1b[1mbacalhau/get.go:67\x1b[0m\x1b[36m >\x1b[0m Fetching results of job 'ab354ccc-f02e-4262-ad0b-f33ec78803cc'...\n2022/09/18 12:19:37 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB). See https://github.com/lucas-clemente/quic-go/wiki/UDP-Receive-Buffer-Size for details.\n\x1b[90m12:19:47.364 |\x1b[0m \x1b[32mINF\x1b[0m \x1b[1mipfs/downloader.go:115\x1b[0m\x1b[36m >\x1b[0m Found 1 result shards, downloading to temporary folder.\n\x1b[90m12:19:51.091 |\x1b[0m \x1b[32mINF\x1b[0m \x1b[1mipfs/downloader.go:195\x1b[0m\x1b[36m >\x1b[0m Combining shard from output volume 'outputs' to final location: '/content/results'\n")),(0,s.kt)("p",null,"After the download has finished you should\nsee the following contents in results directory"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-bash"},"%%bash\nls results/\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"shards  stderr  stdout  volumes\n")),(0,s.kt)("p",null,"The structure of the files and directories will look like this:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},".\n\u251c\u2500\u2500 shards\n\u2502   \u2514\u2500\u2500 job-8e89eb2f-1ae7-4b92-ba72-8abfade02a23-shard-0-host-QmYgxZiySj3MRkwLSL4X2MF5F9f2PMhAE3LV49XkfNL1o3\n\u2502       \u251c\u2500\u2500 exitCode\n\u2502       \u251c\u2500\u2500 stderr\n\u2502       \u2514\u2500\u2500 stdout\n\u251c\u2500\u2500 stderr\n\u251c\u2500\u2500 stdout\n\u2514\u2500\u2500 volumes\n    \u2514\u2500\u2500 outputs\n        \u251c\u2500\u2500 dataset_description.json\n        \u251c\u2500\u2500 sub-01_T1w.html\n        \u251c\u2500\u2500 sub-01_T1w.json\n        \u251c\u2500\u2500 sub-01_task-mixedgamblestask_run-01_bold.html\n        \u251c\u2500\u2500 sub-01_task-mixedgamblestask_run-01_bold.json\n        \u251c\u2500\u2500 sub-01_task-mixedgamblestask_run-02_bold.html\n        \u251c\u2500\u2500 sub-01_task-mixedgamblestask_run-02_bold.json\n        \u251c\u2500\u2500 sub-01_task-mixedgamblestask_run-03_bold.html\n        \u251c\u2500\u2500 sub-01_task-mixedgamblestask_run-03_bold.json\n        \u251c\u2500\u2500 sub-02_T1w.html\n        \u251c\u2500\u2500 sub-02_T1w.json\n        \u251c\u2500\u2500 sub-02_task-mixedgamblestask_run-01_bold.html\n        \u251c\u2500\u2500 sub-02_task-mixedgamblestask_run-01_bold.json\n        \u251c\u2500\u2500 sub-02_task-mixedgamblestask_run-02_bold.html\n        \u251c\u2500\u2500 sub-02_task-mixedgamblestask_run-02_bold.json\n        \u251c\u2500\u2500 sub-02_task-mixedgamblestask_run-03_bold.html\n        \u251c\u2500\u2500 sub-02_task-mixedgamblestask_run-03_bold.json\n        \u251c\u2500\u2500 sub-03_T1w.html\n        \u251c\u2500\u2500 sub-03_T1w.json\n        \u251c\u2500\u2500 sub-03_task-mixedgamblestask_run-01_bold.html\n        \u251c\u2500\u2500 sub-03_task-mixedgamblestask_run-01_bold.json\n        \u251c\u2500\u2500 sub-03_task-mixedgamblestask_run-02_bold.html\n        \u251c\u2500\u2500 sub-03_task-mixedgamblestask_run-02_bold.json\n        \u251c\u2500\u2500 sub-03_task-mixedgamblestask_run-03_bold.html\n        \u2514\u2500\u2500 sub-03_task-mixedgamblestask_run-03_bold.json\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"The outputs of your job is in volumes/outputs\n")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Volumes folder contains the outputs of our job"),(0,s.kt)("li",{parentName:"ul"},"stdout contains things printed to the console like outputs, etc."),(0,s.kt)("li",{parentName:"ul"},"stderr contains any errors. In this case, since there are no errors, it's will be empty")))}d.isMDXComponent=!0}}]);